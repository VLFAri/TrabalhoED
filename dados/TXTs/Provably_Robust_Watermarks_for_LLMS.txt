
Preprint

PROVABLY ROBUST WATERMARKS FOR OPEN-
SOURCE LANGUAGE MODELS

Miranda Christ
Columbia University
mchrist@cs.columbia.edu

Sam Gunn
UC Berkeley
gunn@berkeley.edu

Tal Malkin
Columbia University
tal@cs.columbia.edu

Mariana Raykova
Google
marianar@google.com

ABSTRACT

The recent explosion of high-quality language models has necessitated new meth-
ods for identifying AI-generated text. Watermarking is a leading solution and
could prove to be an essential tool in the age of generative AI. Existing approaches
embed watermarks at inference and crucially rely on the large language model
(LLM) specification and parameters being secret, which makes them inapplica-
ble to the open-source setting. In this work, we introduce the first watermarking
scheme for open-source LLMs. Our scheme works by modifying the parame-
ters of the model, but the watermark can be detected from just the outputs of
the model. Perhaps surprisingly, we prove that our watermarks are unremovable
under certain assumptions about the adversary’s knowledge. To demonstrate the
behavior of our construction under concrete parameter instantiations, we present
experimental results with OPT-6.7B and OPT-1.3B. We demonstrate robustness to
both token substitution and perturbation of the model parameters. We find that the
stronger of these attacks, the model-perturbation attack, requires deteriorating the
quality score to 0 out of 100 in order to bring the detection rate down to 50%.

1 INTRODUCTION

As generative AI becomes increasingly capable and available, reliable solutions for identifying AI-
generated text grow more and more imperative. Without strong identification methods, we face risks
such as model collapse (Shumailov et al., 2024), mass disinformation campaigns (Ryan-Mosley,
2023), and detection false positives leading to false plagiarism accusations (Ghaffary, 2023). Wa-
termarking is a prominent and promising approach to detection. Recent works (Kirchenbauer et al.
(2023); Aaronson (2022); Zhao et al. (2024); Christ et al. (2024); Kuditipudi et al. (2024); Fairoze
et al. (2023); Christ & Gunn (2024)) construct “sampler-based watermarks,” for the setting where
the watermark is embedded at generation time and users have only query access to the model. Such
watermarks modify the algorithm used by the LLM to sample from the probability distribution over
the next token, leaving the underlying neural network untouched. For example, Zhao et al. (2024)
shifts this distribution to prefer sampling tokens on a fixed “green list.” These existing approaches
are ill-suited for scenarios where an attacker has access to the code for the watermarked model and
can simply rewrite the sampling algorithm to sample from the unmodified probability distribution
— yielding the original, unwatermarked model, with no loss in quality.

Such scenarios are gaining importance as open-source models become more widely available and
higher quality (e.g., LLaMA Touvron et al. (2023), Mistral Jiang et al. (2023), OPT Zhang et al.
(2022)). However, watermarks for open-source models have been severely understudied. In this
work we initiate a formal study of such watermarks, in the setting where the model parameters and
associated code are publicly available, and the watermark is embedded by altering the weights of
the neural network. We consider the standard notion of autoregressive language models used in
existing watermarking works (see Appendix A.2 for a precise definition). Constructing watermarks

∗Authors listed in alphabetical order.

1


Preprint

that are not easily removable by an attacker with such comprehensive model access is a significant
challenge.

Our contributions. We put forth a formal definition of an unremovable watermark for neural net-
works, then construct such a watermarking scheme. Unlike sampler-based watermarks, ours mod-
ifies the weights of the model. Our watermark is unremovable in the sense that, under certain
assumptions, it cannot be removed without degrading the quality of the model. Furthermore, our
watermark can be detected not only with access to the weights, but even from a modest amount of
text (∼300 tokens) produced by the model.

Of course, one must place a restriction on the allowable ways in which the adversary may modify
the weights, otherwise it can ignore the watermarked model and generate unwatermarked text of its
own. We do so by imposing a strict quality condition: We prove that if an adversary removes our
watermark, the resulting model is low-quality, assuming that the attacker has some uncertainty about
the distribution of high-quality text. This assumption is necessary because otherwise, the adversary
could simply ignore the watermarked model and train a new model itself. We precisely formalize
this assumption and the tradeoff between quality degradation and watermark strength.

In addition to proving unremovability, we provide a suite of experiments using OPT-6.7B and OPT-
1.3B, showing that our theoretical guarantees translate to practice. We find that the strongest attack
we consider requires deteriorating text quality to zero out of 100 in order to bring the detection rate
to 50%. Our approach is quite general, and our techniques may be of independent interest due to
their applicability in other settings where one wishes to watermark high-dimensional data.

2 TECHNICAL OVERVIEW

Formalizing unremovability (Section 4.1). Recall that the goal of the adversary is to take a wa-
termarked model and produce a high-quality unwatermarked model. One cannot hope for a water-
mark to be unremovable by an adversary with sufficient knowledge to train its own unwatermarked
model. Therefore, we must consider only adversaries with limited knowledge about the distribution
of high-quality text. We formalize this notion as follows: we model ideal-quality language as being
described by an original neural network M∗. That is, the ideal-quality language is the distribution of
text that would be produced by an LLM using M∗ to compute the probability distribution over each
token. This neural network M∗ is then watermarked, and the resulting M ′ is given to the adversary.
The adversary’s uncertainty is captured by its posterior distribution over M∗, given M ′.

We assume that the adversary’s posterior distribution over M∗ is normally distributed in a particular
representation space of models, and centered at M ′. Under this assumption, we prove that there is a
steep trade-off between the magnitude of changes the adversary must make to remove the watermark,
and the magnitude of changes required to add the watermark. That is, text produced by the adversary
with access to M ′ is either watermarked or low-quality.

Our scheme (Section 5). Our scheme is quite natural: We modify the bias of each neuron in the
last layer of the model by adding a perturbation from N (0, ε2), as described in Algorithms 1 and 2.
The watermarking detection key is the vector of these perturbations. Given the weights of a model,
we can detect the watermark by checking the correlation between the biases of its last layer and
the watermark perturbations. More precisely, we compute the inner product between the watermark
perturbations, and the difference between the biases of the given model and the original model
(Algorithm 3).

Furthermore, we observe that it is possible to approximate this inner product given only text pro-
duced by the model. This text detector (Algorithm 4) computes a score that is the sum of the
watermark perturbations corresponding to the distinct tokens observed in the text. We show that
in expectation, this score is approximately the inner product from the detector from weights (Algo-
rithm 3), where the approximation error is lower for higher-entropy text. We prove that in sufficiently
high-entropy text, our text detector succeeds with overwhelming probability.

Proving unremovability (Theorem 3). We show unremovability in two steps: (1) Any high-quality
model produced by altering the watermarked model must have a high inner product score (Theo-
rem 1), and (2) If a language model has a high inner product score, its responses are watermarked
(Theorem 3).

2



Preprint

To understand the intuition behind (1), consider the watermarked and unwatermarked models as
vectors w⃗wat, w⃗

∗ ∈ Rn, respectively. The adversary is given w⃗wat, and it aims to produce z ∈ Rn

that is unwatermarked, but that is sufficienty close to w⃗∗ (i.e., has high quality). We can describe
the region of watermarked vectors geometrically: This region is essentially a halfspace, where the
hyperplane describing it lies between w⃗wat and w⃗∗, and it is orthogonal to the line from w⃗wat to w⃗∗.
Removing the watermark involves crossing this hyperplane. If w⃗∗ is known, the most efficient way
to remove the watermark is to add to w⃗∗ in the direction of (w⃗∗ − w⃗wat). However, because the
adversary’s posterior distribution over w⃗∗ is centered at w⃗wat, the adversary is unlikely to produce
a short vector (z − w⃗wat) that moves far in the direction of (w⃗∗ − w⃗wat): We expect its weight in
this direction to be proportional to its length. This allows us to show that, if z is not watermarked,
then (z − w⃗wat) has large magnitude. If (z − w⃗wat) has large magnitude, then z is far from w⃗∗ and
therefore low quality.

Now consider some text produced by a model on the watermarked side of the hyperplane. For each
token, if the watermark increases its bias relative to that of w⃗∗, the model is more likely to output that
token. Prior work of Zhao et al. (2024) uses this principle to obtain a simple “counting” detector,
which essentially computes the fraction of positive-bias tokens in the given response. However,
we observe that because our watermark alters different token biases by different amounts, we can
substantially improve detectability by taking the magnitudes of the perturbations into account in
our detector. Our detector computes a score, which is the sum of bias perturbations of all observed
tokens in the text (in contrast to the counting detector, which is the sum of indicators of the bias
perturbations’ signs). Interestingly, we prove that our text detector, when applied to text produced
by z, approximates the inner product between (z− w⃗∗) and (w⃗wat− w⃗∗). Because this inner product
is high for watermarked z and roughly 0 for independently generated z, our detector is reliable and
has a negligible false positive rate. Similarly to other watermarks (e.g., Kirchenbauer et al. (2023);
Christ et al. (2024); Zhao et al. (2024)) our proof of detectability from text (2) relies on sufficient
entropy in the text.

When proving unremovability of the text watermark (2), we assume that the adversary alters only the
biases in the last layer of the watermarked model it is given. We argue that an adversary successfully
making arbitrary alterations to the model could use this new model to learn ways to alter only the
biases of the watermarked model. We leave a formal expansion of the class of adversaries to future
work.

Experiments (Section 6). We show that our watermark is indeed detectable and unremovable in
practice, using experiments run on OPT-6.7B and OPT-1.3B (Zhang et al., 2022). We use Mistral-
7B Instruct (Jiang et al., 2023) to measure the quality of responses, and we show that the wa-
termarked models with our scheme preserve similar quality to their unwatermarked version. We
demonstrate reliable detectability rates for our construction, given ∼300-token responses of various
types. In addition to our proofs for unremovability, we consider a number of concrete adversaries
and demonstrate that the watermark persists in the adversarially altered watermarked models unless
the adversary adds large perturbations that significantly reduce the quality of the resulting model.

3 RELATED WORK

We describe the most relevant related works here, but refer readers to Tang et al. (2023); Piet et al.
(2023) for more comprehensive surveys.

Existing watermarks change the sampling function of the LLM, and are therefore easily removable
given code for this sampling function. Aaronson (2022); Kirchenbauer et al. (2023); Zhao et al.
(2024); Christ et al. (2024); Fairoze et al. (2023) sample each token by partitioning the token set into
red and green lists (that depends on the previously-output tokens), then increasing the probability of
the tokens in the green list. To detect the watermark, one computes the fraction of green tokens in
the given text and compares it to a threshold. Kuditipudi et al. (2024); Christ & Gunn (2024) operate
slightly differently—for each response, they choose a random seed. In Kuditipudi et al. (2024) there
are polynomially many possible seeds for a given watermarking key, and in Christ & Gunn (2024)
there are exponentially many. They then choose the ith token of the response to be correlated with
the ith value of the random seed. To detect the watermark, one essentially computes the correlation
between the response and the seed. While not unremovable, some of these watermarks satisfy a
weaker robustness property: Given a watermarked response, it is difficult to make a bounded number
of edits to yield an unwatermarked text.

3



Preprint

Our scheme is most similar in spirit to that of Zhao et al. (2024), though theirs is a sampler-based
watermark. They choose a fixed red/green partition that is used to change the sampling function for
all responses. Similarly, we fix the partition of tokens whose biases we increase and decrease. The
two major differences are that (1) we make this change by altering the weights of the model, and (2)
while Zhao et al. (2024) increases the logits of all green tokens by the same amount, our increases
are normally distributed. This normal distribution is crucial for unremovability, and it results in a
different detector being most effective for our scheme. The detector of Zhao et al. (2024) simply
computes the fraction of green tokens, while our inner product detector computes a score that is
essentially a weighted count of the number of green tokens. These weights correspond to the size of
the perturbations added by our watermark.

The works closest to our setting are Gu et al. (2024); Sander et al. (2024), which show empirically
that certain sampler based watermarks can be learned. That is, a model trained on watermarked
texts may learn to generate watermarked responses itself. The watermark therefore is embedded in
the weights of the model; however, there is no unremovability guarantee, and the detectability of
these responses is much lower than their sampler-based counterparts. In fact, Gu et al. (2024) finds
empirically that the watermark is destroyed after the model is fine-tuned and leaves the question of
unremovable open-source watermarks for future work.

We also note that prior work Zhang et al. (2024) shows that any LLM watermark is removable by
a sufficiently strong adversary. This underscores our need to consider an adversary with limited
knowledge about high-quality language when defining and showing unremovability.

4 PRELIMINARIES AND DEFINITIONS

Let N := {1, 2, . . . } denote the set of positive integers. We will write [q] := {1, . . . , q}. For a set
X , we define X∗ := {(x1, . . . , xk) | x1, . . . , xk ∈ X ∧ k ∈ Z≥0} to be the set of all strings with
alphabet X . For a binary string s ∈ X∗, we let si denote the ith symbol of s and lens denote the
length of s. For a string s ∈ X∗ and positive integers a ≤ b ≤ lens, let s[a : b] denote the substring
(sa, . . . , sb). We use log(x) to denote the logarithm base 2 of x, and ln(x) to denote the natural
logarithm of x.

For a finite set X , we will use the notation x ← X to denote a uniformly random sample x from
X . If X is a set of n-dimensional column vectors, we will write Xm to refer to the set of n ×m
matrices whose columns take values in X . Unless otherwise specified, vectors are assumed to be
column vectors. For a vector x ∈ Rn, we let ∥x∥ = ∥x∥2 =

√∑n
i=1 x

2
i .

Let Ber(p) be the Bernoulli distribution on {0, 1} with expectation p. Let Ber(n, p) be the distri-
bution on n-bit strings where each bit is an i.i.d sample from Ber(p). For a distribution D, we let
Supp(D) denote its support.

Let λ denote the security parameter. A function f of λ is negligible if f(λ) = O( 1
poly(λ) ) for every

polynomial poly(·). We write f(λ) ≤ negl(λ) to mean that f is negligible. We say a probability
is overwhelming in λ if it is equal to 1 − f for some negligible function f . We let ≈ denote
computational indistinguishability and ≡ denote statistical indistinguishability.

We provide a formal description of a language model in Appendix A.2.

4.1 WATERMARKS

We present general definitions for watermarking content from some content distribution C of real
vectors; that is, Supp(C) ⊆ Rn. We will later use this real-content watermark framework to water-
mark the weights of a neural network.

Definition 1 (Watermark). A watermark is a tuple of polynomial-time algorithms W =
(Setup,Watermark,Detect) such that:

• Setup(1λ)→ sk outputs a secret key, with respect to a security parameter λ.

• Watermarksk(x) → x′ is a randomized algorithm that takes as input some content x and
outputs some watermarked content x′.

• Detectsk(x
′) → {true, false} is an algorithm that takes as input some content x′ and out-

puts true or false.

4



Preprint

Definition 2 (Quality loss function). A quality loss function for content from Rn is a function
L : Rn × Rn → R. We say that L(x, y) is the quality loss score of the content y ∈ Rn relative to
some ideal content x.

The following removability game is defined over a content distribution C, a quality measure L over
Rn, and a loss parameter ℓ(·) : Z→ R.

Definition 3 ((C, L, ℓ)-Removability game Gremov
A,W,C(1

λ, L, ℓ)). Let C be a content distribution. The
removability game Gremov

A,W,C(1
λ, L, ℓ) is defined between an adversaryA and a challenger as follows.

1. The challenger runs sk← Setup(1λ).

2. The challenger chooses some original content x ← C and produces watermarked content
xwat ←Watermarksk(x).

3. A receives xwat and produces x′.

4. The adversary wins if x′ is not watermarked, and its quality loss is acceptable: L(x, x′) ≤
ℓ(n).

The output of the game Gremov
A,W,C(1

λ, C, L, ℓ) is 1 if and only if the adversary wins.

Definition 4 (Unremovability). LetW be a watermark for content in Rn. Let C be a content distri-
bution, Q be a quality loss function over Rn, and ℓ(·) : Rn × Rn → R be a loss parameter. We say
thatW is (C, L, ℓ)-unremovable if for all p.p.t. adversaries A,

Pr
[
Gremov
A,W,C(1

λ, L, ℓ) = 1
]
≤ negl(λ).

Definition 5 (Soundness/low false positive rate). Let W be a watermark for content in Rn. W is
sound if for any fixed x ∈ Rn,

Pr
sk←Setup(1λ)

[Detectsk(x) = true] ≤ negl(λ).

As shorthand, when the setup and watermark algorithms are clear from context, we sometimes write
that Detect (rather thanW) is unremovable or sound.

5 WATERMARKING SCHEME

We first show a general watermarking scheme for vectors in Rn. We then show how to apply this
paradigm to neural networks, by watermarking the biases of the output layer. Our watermark detec-
tor is strongest when given explicit access to the weights of the watermarked model (Section 5.1).
Furthermore, for a language model, where these biases directly affect the distribution of tokens in its
outputs, our watermark is even detectable in text produced by a watermarked model (Section 5.2).

5.1 DETECTING THE WATERMARK FROM THE WEIGHTS OF THE MODEL

We consider an arbitrary vector w⃗∗ in Rn to which Gaussian noise is added, to obtain w⃗wat. Observe
that the inner product between (w⃗wat − w⃗∗) and the vector of the Gaussian perturbations ∆ is large.
Naturally, our detector WeightDetect (Algorithm 3) computes this inner product. We show that any
adversary whose posterior distribution (after seeing w⃗wat) over w⃗∗ is Gaussian cannot remove the
watermark without adding significant perturbations of its own. In particular, if w⃗∗ has dimension
n, the adversary must add a vector of Euclidean norm Ω(n/

√
log n) to succeed at removing the

watermark. In contrast, embedding the watermark involves adding a perturbation of Euclidean norm
only O(

√
n): Watermark removal requires a change that is larger than watermark embedding by a

factor of nearly
√
n. So far, this approach is general to watermarking any real vector. Although our

work focuses on applying it to LLMs, it is likely that this paradigm can be applied to other scenarios,
which we leave for future work.

To apply our watermark to LLMs, we will later take w⃗∗ to be the biases in the last layer of a neural
network, making n the size of the token alphabet.

Definition 6 (L2: Euclidean quality loss function.). Let L2 : Rn × Rn → R be the quality loss
function that, on input x, y ∈ Rn, outputs ∥x− y∥2. We call L2 the Euclidean loss function.

5



Preprint

Algorithm 1: Watermarked content generator Setup
Result: Watermark secret key ∆ ∈ Rn

1 return ∆← N (0, ε2I);

Algorithm 2: Watermarked content generator Watermark

Input: Content x ∈ Rn and watermark secret key ∆ ∈ Rn

Result: Watermarked content x′ ∈ Rn and original content x
1 x′ ← x+∆;
2 return x′, x;

Algorithm 3: Watermark detector
WeightDetect∆
Input: Content c ∈ Rn, original content

x ∈ Rn, and a detection key
∆ ∈ Rn

Result: true or false depending on whether
c is watermarked

1 if (c− x) ·∆ ≥ τε2n and
∥c− (x+∆)∥ ≤ 1

2ε
2n then

2 return true;
3 return false;

Algorithm 4: Watermark detector
TextDetect∆
Input: Text x1, . . . , xℓ ∈ T and detection

key ∆ ∈ Rn

Result: true or false depending on whether
the text is watermarked

1 S ← ∅;
2 count← 0;
3 for i ∈ [ℓ] do
4 if xi /∈ S then
5 count← count+∆(xi);
6 S ← S ∪ {xi};
7 if |S| ≥ λ and count ≥ |S|ε2τtext then
8 return true;
9 return false;

Figure 1: The algorithmsW = (Setup,Watermark,WeightDetect,TextDetect).

Theorem 1. Let I be the n × n identity matrix, and let C be such that the adversary’s posterior
distribution over the original content w⃗∗ after seeing w⃗wat is N (0, ε2I).

Let the loss parameter be ℓ(n) := εn√
logn

. Then WeightDetect is (C, L2, ℓ)-unremovable.

Theorem 2. WeightDetect is sound.

We defer the proofs of Theorems 1 and 2 to Appendices A.3.1 and A.3.2 respectively.

5.2 DETECTING THE WATERMARK FROM TEXT

In this section, we will show that the inner product from WeightDetect can be approximated given
text. Our detector from text, TextDetect (Algorithm 4), simply computes the sum of the pertur-
bations of the biases of tokens in the given text. We observe that a positive perturbation increases
the probability of outputting the given token, and the magnitude of the perturbation corresponds
to the magnitude of the increase. Similarly, a negative perturbation decreases a token’s likelihood.
Therefore, we expect the frequency of a token in a watermarked response to have an observable
correlation with the sign and magnitude of the perturbation added to its bias.

We prove in Theorem 3 that this intuition is indeed correct. In particular, this sum of perturbations
of observed tokens approximates a value that is 0 in expectation for natural text, and grows linearly
with the text length for watermarked text. This is true even when the watermarked model is modified
by an adversary attempting to remove the watermark. The accuracy of the detector’s approximation
depends on the entropy of the text, and the quality of the model produced by the adversary. That is,
low-entropy responses and low-quality adversarial models will have lower watermark detectability.

We first introduce some notation and recall the structure of a language model. Let z be the biases of
the model produced by an adversary modifying the watermarked model w⃗wat. Let pit and qit be the
probabilities that the models z and w⃗∗, respectively, assign to token t ∈ [n] at step i. Let ℓir be the

6



Preprint

logit of token r in step i, under the original model w⃗∗. Therefore,

pit =
eℓ

i
t+zt∑

r∈[n] e
ℓir+zr

and qit =
eℓ

i
t+w⃗∗

t∑
r∈[n] e

ℓir+w⃗∗
r

Recall that language models compute their probability distribution using the softmax function.
Therefore, no probabilities are actually zero but are rather extremely small values, which are prac-
tically zero. We assume there is some threshold for which probabilities above that threshold are
significant, and probabilities below that threshold are functionally zero—in practice, those tokens
are not sampled. We treat these negligible probabilities as zero. We will use the following assump-
tions:

Assumption 1 (Similar normalization).
∑

r∈[n] e
ℓir+zr ≈

∑
r∈[n] e

ℓir+w⃗∗
r .

Assumption 2 (Derived from similar normalization, with concrete approximation error). There
exists a very small constant η such that for every t ∈ [n],

pit/q
i
t − 1− η ≤ (zt − w⃗∗t ) ≤ pit/q

i
t − 1 + η.

Assumption 1 states that the under the adversary’s model and the original model, the normalization
factors used in the softmax function are roughly equal. Intuitively, for an attack that significantly
changes this normalization factor, there is a similar attack that makes smaller renormalized changes.
For example, if the adversary increases all biases by a large amount, it might as well change them
by a smaller amount with the same ratios between them. Furthermore, the quality requirement
bounds the adversary’s changes zr. Assumption 2 is derived from Assumption 1 and an additional
approximation that ex ≈ 1+x. We make this approximation error concrete by introducing η, which
we assume to be small. We show this derivation in the proof of Lemma 2.

Definition 7 (c1-high entropy). Let T ⊆ [n] be the subset of tokens over which pi has significant
probability. We say that pi has c1-high entropy for c1 > 0 if for all t ∈ T , pit ≥ 1

c1|T | .

In other words, Definition 7 states that for tokens with significant probability under pi, their proba-
bilities are within a constant factor of 1/|T |. That is, pi is somewhat close to uniform over T .

Definition 8 (c2-high quality). Let T ⊆ [n] be the subset of tokens over which pi has significant
probability. We say that pi has c2-high quality for c2 > 0 if for all t ∈ T , qit ≥ pitc2.

c2-high quality states that the probabilities under the original model are within a constant factor of
the probabilities under the adversary’s model. This is true if the adversary makes bounded changes—
recall that pit ≈ qit · e(zt−w⃗

∗
t ). Therefore, it is true if (zt − w⃗∗t ) ≤ − log c2.

Observe that c1-high entropy and c2-high quality together imply the following fact:

Fact 1. Let pi be c1-high entropy and c2-high quality, and let T be the set of tokens with non-
negligible probability under pi. Then for all t ∈ T , 1/qit ≤

c1|T |
c2

.

Lemma 2. Let x1, . . . , xk be any sequence of tokens generated using a model produced by the
adversary. For all i ∈ [k] with c1-high entropy (Definition 7) and c2-high quality (Definition 8),
under reasonable assumptions (Assumptions 1 and 2), we have:

E[(w⃗wat − w⃗∗)xi
] ≥ c1ε

2/c2 − α,

where α =
c2ηε
√

2/π

c1
is a very small approximation error term.

We defer the proof of Lemma 2 to Appendix A.3.3.

Assumption 3 (Independence). Consider a response output by the adversary’s model, and let
x1, . . . , xk be a subsequence of distinct tokens in this response. The random variables (w⃗wat−w⃗∗)xi

are independent.

7



Preprint

This assumption is concerned with a degenerate case where the fact that a model output tokens with
positive perturbation (e.g., (w⃗wat − w⃗∗)xi > 0) makes it less likely to output positive-perturbation
tokens in the future. This is similar to an assumption used in Kirchenbauer et al. (2023) that the
green list is freshly drawn for each token in the response; and an assumption used in Zhao et al.
(2024) called “homophily”.

Our main theorem states that if an adversary is given a watermarked model and it arbitrarily modifies
its biases, the resulting model still produces watermarked text with overwhelming probability. This
probability increases as ε and the number of distinct tokens increase.

Theorem 3. TextDetect detects the watermark given an adversarially produced text with enough
distinct tokens, enough entropy, and high enough quality.

More precisely, let I be the n × n identity matrix, and let C be such that the adversary’s posterior
distribution over the original content x after seeing xwat is N (0, ε2I).

Let the adversary produce a model and a response generated by that model such that:

• The response contains at least λ distinct tokens.

• At least a constant δ > 0 fraction of the distributions pi of these distinct tokens are c1-

high entropy and c2-high quality, for constants c1, c2 > 0. We let α =
c2ηε
√

2/π

c1
be the

corresponding approximation error term.

Under reasonable assumptions (Assumptions 1 to 3), TextDetect with threshold τtext = δc2/2c1 −
α/2 outputs true with overwhelming probability.

Theorem 4. For any constant threshold τtext > 0, TextDetect is sound.

We defer the proofs of Theorems 3 and 4 to Appendices A.3.4 and A.3.5 respectively.

6 EXPERIMENTAL EVALUATION

In this section we present the results of an experimental evaluation of our watermarking construction
which demonstrate its behavior under concrete parameter settings. Our experiments aim to show
feasibility rather than optimality, as we open the door to further open-source watermarking work
with experimental findings consistent with our theoretical results.

Experimental Setup. We watermark two LLMs: the OPT model with 1.3B and 6.7B parameters.
We generate responses of three types: story, essay, and code; we include these prompts and addi-
tional prompt parameters in Appendix A.4. In all of our plots, we compute the fraction of responses
detected among those with at least 20 distinct tokens, for a range of ε parameter values. Recall
that ε is the standard deviation of the Gaussian perturbations added to the biases. Therefore, the
detectability of the watermark increases with epsilon.

Detectability. In Figure 2, we plot the fraction of watermarked responses that are detected at 1%
and 5% false positive rates. These responses are output by a model watermarked using our scheme,
without further modification. The curves for “Inner Product Detector” show detectability under our
detector TextDetect from Algorithm 4. Observe that for ε ≥ 0.5, we have a true positive rate
of at least 80%. Furthermore, this plot supports our result (Theorem 3) that the true positive rate
increases with ε. We emphasize that although our detectability is not as strong as some inference-
time watermarks, ours is the only open-source watermark with a provable unremovability guarantee.

We also plot detectability for an alternate detector (“Count Detector”). This alternate detector is
analogous to that of Kirchenbauer et al. (2023); Zhao et al. (2024) in that it computes the fraction
of tokens whose perturbations were positive (e.g., green list tokens). Observe that this detector has
a significantly lower true positive rate than of TextDetect for our scheme. This shows that the
open-source setting requires new techniques for reliable detection, and that existing inference-time
detectors do not carry over even when implementable in the weights of the model. This supports our
strategy of considering the magnitudes of the perturbations during detection, in our Inner Product
Detector.

8



Preprint

Figure 2: True positive detection rates for responses generated by OPT-1.3B, with our watermark
applied under varying perturbation magnitudes (epsilon). The “Inner Product Detector” is our
TextDetect detector (Algorithm 4), and the “Count Detector” is a baseline detector that we compare
to.

(a) (b)

Figure 3: (a): Detection rates for adversarially perturbed watermarked OPT-6.7B models, to simulate
a removal attack. The x-axis plots the epsilon parameter used in the watermark. The three curves
show detection rates for models obtained by adding additional Gaussian noise to the biases, of
standard deviaton 1, 2, and 5 times epsilon.
(b): Quality scores of watermarked texts generated with various parameters of epsilon, under the
same perturbation attacks. The quality score of unwatermarked text was 59.933.

9



Preprint

Figure 4: Detection rates for texts produced by a watermarked OPT-6.7B model and subjected to
a substitution attack. The x-axis plots the fraction of tokens that are substituted. The curves show
detection rates for varying epsilon parameters of the watermark.

Unremovability. While we prove the unremovability properties of our watermark in Ap-
pendix A.3.4, in this section we consider a concrete attack adversary and show the unremovability-
quality trade-off for it. The adversary produces a new model by adding Gaussian perturbations of
mean zero and standard deviation 1x epsilon, 2x epsilon, or 5x epsilon. In Figure 3, we plot detec-
tion rates and quality scores for responses generated by adversarially modified watermarked models.
That is, a watermarked model is produced at the given parameter epsilon shown on the x-axis.

In Figure 3a, we observe that in the 1x attack, for ε ≥ 0.6 the detection rate remains above 80%. The
watermark fails to persist in the larger-perturbation attacks in part because for large perturbations,
the model produces highly repetitive responses, yielding few distinct tokens and a weak watermark
signal. One can see this reflected in the quality scores shown in Figure 3b. The 5x attack, shown in
red, has nearly zero quality for values of epsilon at least 0.6. The 2x attack, reaches zero quality at
epsilon=1, at which point the watermark is still detectable with at least 50% probability.

In Figure 4, we show detection rates for watermarked responses subjected to a token substitution
attack, as considered in Kuditipudi et al. (2024). We reproduce their attack, choosing a random
subset of tokens in the response to substitute with uniform tokens from the token alphabet. We
observe that our watermark tolerates moderate substitution attacks.

Additional discussion. Our observed tradeoffs were affected by the phenomenon that large pertur-
bations caused the model’s responses to be very repetitive, therefore containing few distinct tokens.
While higher values of epsilon (e.g., larger perturbations) increase the strength of our watermark,
fewer distinct tokens decrease the strength of our watermark. This can be seen especially under the
unremovability experiment where the adversary perturbs the model at 5x epsilon Figure 3. Here, the
adversary’s large perturbations caused the model to deteriorate so profoundly that most responses
contained fewer than 20 distinct tokens, resulting in detection failure.

While our watermark is detectable in practice, and it exhibits an unremovability-quality tradeoff,
the strong asymptotic tradeoff in Section 5 is somewhat dampened by the concrete parameters we
consider. For example, the false negative rate is exponential in −ε4T , where T is the number of
distinct tokens. Although asymptotically this expression is negligible in the number of distinct
tokens, for concrete values of ε this term dominates. For example, suppose that the watermark
was applied with ε = 0.3. We show that the false negative rate is at most exp(−(0.3)4T ), where
T is the number of distinct tokens; to get any meaningful false positive guarantee we must have
T ≥ (0.3)−4 ≈ 125. The vast majority of responses generated in our experiments had fewer than
100 distinct tokens, because we aimed to measure the detectability of paragraph-length texts of
∼ 300 tokens.

Quality. In Figure 5, we plot quality scores for text generated by OPT-1.3B, using our watermark
with various parameters of epsilon. We compute these quality scores by asking Mistral-Instruct-
7B to provide a score out of 100. We report the average scores across 60 responses per value of

10



Preprint

epsilon, including responses of the three types we consider (essay, story, and code). Unwatermarked
responses had an average quality score of 59.933.

ACKNOWLEDGMENTS

Sam Gunn was supported by a Google PhD fellowship. Miranda Christ and Tal Malkin were sup-
ported by a Google CyberNYC grant, an Amazon Research Award, and an NSF grant CCF-2312242.
Miranda Christ was additionally supported by NSF grants CCF-2107187 and CCF-2212233. This
work was done while Miranda Christ was a student researcher at Google. Any opinions, findings,
and conclusions or recommendations expressed in this material are those of the authors.

REFERENCES

Scott Aaronson. My AI Safety Lecture for UT Effective Altruism. https://scottaaronson.
blog/?p=6823, November 2022. Accessed May 2023.

Miranda Christ and Sam Gunn. Pseudorandom error-correcting codes. In Leonid Reyzin and Dou-
glas Stebila (eds.), Advances in Cryptology - CRYPTO 2024 - 44th Annual International Cryp-
tology Conference, Santa Barbara, CA, USA, August 18-22, 2024, Proceedings, Part VI, vol-
ume 14925 of Lecture Notes in Computer Science, pp. 325–347. Springer, 2024. doi: 10.1007/
978-3-031-68391-6\ 10. URL https://doi.org/10.1007/978-3-031-68391-6_
10.

Miranda Christ, Sam Gunn, and Or Zamir. Undetectable watermarks for language models. In Shipra
Agrawal and Aaron Roth (eds.), The Thirty Seventh Annual Conference on Learning Theory, June
30 - July 3, 2023, Edmonton, Canada, volume 247 of Proceedings of Machine Learning Re-
search, pp. 1125–1139. PMLR, 2024. URL https://proceedings.mlr.press/v247/
christ24a.html.

Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, and
Mingyuan Wang. Publicly detectable watermarking for language models. CoRR, abs/2310.18491,
2023. doi: 10.48550/ARXIV.2310.18491. URL https://doi.org/10.48550/arXiv.
2310.18491.

Shirin Ghaffary. Universities rethink using ai writing detectors to vet students’ work. Bloomberg
News, September 2023.

Chenchen Gu, Xiang Lisa Li, Percy Liang, and Tatsunori Hashimoto. On the learnability of
watermarks for language models. In The Twelfth International Conference on Learning Rep-
resentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL
https://openreview.net/forum?id=9k0krNzvlV.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,
Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.
Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A
watermark for large language models. In International Conference on Machine Learning, pp.
17061–17084. PMLR, 2023.

Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-free
watermarks for language models. Trans. Mach. Learn. Res., 2024, 2024. URL https://
openreview.net/forum?id=FpaCL1MO2C.

Julien Piet, Chawin Sitawarin, Vivian Fang, Norman Mu, and David Wagner. Mark my words:
Analyzing and evaluating language model watermarks. arXiv preprint arXiv:2312.00273, 2023.

Tate Ryan-Mosley. How generative ai is boosting the spread of disinformation and propaganda. MIT
Technology Review, October 2023.

Tom Sander, Pierre Fernandez, Alain Durmus, Matthijs Douze, and Teddy Furon. Watermarking
makes language models radioactive. CoRR, abs/2402.14904, 2024. URL https://doi.org/
10.48550/arXiv.2402.14904.

11

https://scottaaronson.blog/?p=6823
https://scottaaronson.blog/?p=6823
https://doi.org/10.1007/978-3-031-68391-6_10
https://doi.org/10.1007/978-3-031-68391-6_10
https://proceedings.mlr.press/v247/christ24a.html
https://proceedings.mlr.press/v247/christ24a.html
https://doi.org/10.48550/arXiv.2310.18491
https://doi.org/10.48550/arXiv.2310.18491
https://openreview.net/forum?id=9k0krNzvlV
https://openreview.net/forum?id=FpaCL1MO2C
https://openreview.net/forum?id=FpaCL1MO2C
https://doi.org/10.48550/arXiv.2402.14904
https://doi.org/10.48550/arXiv.2402.14904


Preprint

Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal.
Ai models collapse when trained on recursively generated data. Nature, 631(8022):755–759,
2024.

Ruixiang Tang, Yu-Neng Chuang, and Xia Hu. The science of detecting LLM-generated texts. arXiv
preprint arXiv:2303.07205, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Hanlin Zhang, Benjamin L. Edelman, Danilo Francati, Daniele Venturi, Giuseppe Ateniese, and
Boaz Barak. Watermarks in the sand: Impossibility of strong watermarking for language mod-
els. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria,
July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=
bM2s12t4hR.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-
pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer
language models. arXiv preprint arXiv:2205.01068, 2022.

Xuandong Zhao, Prabhanjan Vijendra Ananth, Lei Li, and Yu-Xiang Wang. Provable robust wa-
termarking for ai-generated text. In The Twelfth International Conference on Learning Rep-
resentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL
https://openreview.net/forum?id=SsmT8aO45L.

A APPENDIX

A.1 CONCENTRATION BOUNDS

Here, we use a standard Gaussian vector of dimension n to mean a vector that whose components
are independent Gaussians with unit variance.

Fact 3 (Theorem 2.7). Let x⃗ be an iid standard Gaussian random vector of dimension n. For any
c ∈ (0, 1), we have

Pr
[
n(1− c) < ∥x⃗∥22 < n(1 + c)

]
≥ 1− 2 exp

(
−nc2

8

)
.

Fact 4 (Gaussian tail bound). Let X ∼ N (0, σ2). For any t ≥ 0,

Pr[|X| ≥ t] ≤ 2 exp

(
−t2

2σ2

)
.

Fact 5 (Hoeffding’s Inequality). Let X1, . . . , Xk be independent random variables in [a, b], and let
X =

∑k
i=1 Xi. Then for any constant δ ≥ 0,

Pr [|X − E[X]| ≥ δ] ≤ 2 exp

(
−δ2∑k

i=1(b− a)2

)
.

A.2 LANGUAGE MODELS

A language model operates over a token alphabet T = {t1, . . . , tn}. We sometimes identify tokens
ti with their indices i. We consider autoregressive language models that apply the softmax function
to obtain a probability distribution over the next token. That is, given a prompt and tokens output so
far, the model computes a probability distribution p = [p1, . . . , pn] over the next token as follows.
The last layer of the model computes logits ℓi, . . . , ℓn for each token. Each logit is computed as

ℓi = wi,0 +

m∑
j=1

wi,jvi,j

12

https://openreview.net/forum?id=bM2s12t4hR
https://openreview.net/forum?id=bM2s12t4hR
https://openreview.net/forum?id=SsmT8aO45L
https://cims.nyu.edu/~cfgranda/pages/OBDA_fall17/notes/randomness.pdf


Preprint

where wi,0 is the bias and
∑m

j=1 wi,jvi,j is a weighted average of that node’s inputs. The probability
pi is computed by applying the softmax function to the logits; that is,

pi =
eℓi∑n
j=1 e

ℓj
.

Because the probabilities are computed using the softmax function, no probabilities are exactly equal
to zero; rather, they are extremely small. However, in practice these extremely small probabilities
are functionally zero. We formally treat these small probabilities as cryptographically negligible.

A.3 DEFERRED PROOFS

A.3.1 PROOF OF THEOREM 1
Proof. In the removability game, let w⃗∗ denote the original content and w⃗wat denote the water-
marked content. By our assumption about the adversary’s posterior distribution over w⃗∗, we have
that from the adversary’s perspective, w⃗∗ ∼ N (0, ε2I). We will show that for any z produced by
the adversary, either z is far from w⃗∗, or z is watermarked with high probability.

For any fixed z ∈ Rn, letting v = w⃗∗ − w⃗wat ∼ N (0, ε2I) and u = z − w⃗wat,

(z − w⃗∗) · (w⃗wat − w⃗∗) = (v − u) · v.

We now argue that the quantity (v− u) · v is likely to be large. Let v1 denote the first component of
v, let δ = 1√

n
∥u∥2, and let f > 1 be a constant.

Pr
v←N (0,ε2I)

[(v − u) · v < τε2n] = Pr
v←N (0,ε2I)

[
∥v∥22 − δv1

√
n < τε2n

]
≤ Pr

v←N (0,ε2I)

[
∥v∥22 − δεf

√
n < τε2n and v1 ≤ εf

]
+ Pr

v←N (0,ε2I)
[v1 > εf ]

≤ Pr
v←N (0,ε2I)

[
∥v∥22 − δεf

√
n < τε2n

]
+

1

2π
exp

(
−f2/2

)
= Pr

[
ε2∥N (0, I)∥22 < τε2n+ δεf

√
n
]
+

1

2π
exp

(
−f2/2

)
= Pr

[
∥N (0, I)∥22 <

(
τ +

δf

ε
√
n

)
n

]
+

1

2π
exp

(
−f2/2

)
≤ 2 exp

(
−n
[
1− τ − δf

ε
√
n

]2
/8

)
+

1

2π
exp

(
−f2/2

)
(1)

Above, we made use of the facts that v ∼ N (0, ε2I) ∼ εN (0, I), and v1 ∼ N (0, ε2). We also
made use of the fact that for all ℓ ∈ R and u ∈ Rn,

Pr
v←N (0,ε2I)

[u · v = ℓ] = Pr
v1←N (0,ε2)

[v1∥u∥2 = ℓ] ,

which follows from spherical symmetry of a Gaussian.

We’ve now upper bounded the probability that (v − u) · v is large in Equation (1); it remains to
analyze this probability bound. Observe that for any f = ω(

√
log n), in Equation (1), 1

2π e
−f2/2 is

negligible. For any δ = o(εn/f) = o(εn/
√
log n), in Equation (1) we have that τn− δf

√
n

ε is at least

(1+c)n for some constant c = τ−1−o(1). By Fact 3, Pr
[
∥N (0, I)∥22 ≤ τn− δf

√
n

ε

]
≤ negl(n).

Therefore, the expression in Equation (1) is negligible in n.

Applying the above, if z is unwatermarked with non-negligible probability we must have
∥z − w⃗wat∥2 ≥ ω

(
εn√
logn

)
. By Fact 3, for any constant α, Pr

[
∥w⃗wat − w⃗∗∥2 ≥

αεn√
logn

]
≤

negl(n). By a union bound, if z is unwatermarked then with overwhelming probability we must

13



Preprint

have ∥z − w⃗wat∥2 ≥ ω
(

εn√
logn

)
and ∥w⃗wat − w⃗∗∥2 < αεn√

logn
. By a triangle inequality,

∥z − w⃗∗∥2 ≥ ∥z − w⃗wat∥2 − ∥w⃗wat − w⃗∗∥2

≥ ω

(
εn√
log n

)
− αεn√

log n

≥ ω

(
εn√
log n

)
.

Finally, again by Fact 3, with overwhelming probability ∥w⃗wat − w⃗∗∥2 = Θ(ε
√
n). Therefore,

∥z − w⃗∗∥2 ≥ ω
( √

n√
logn

)
.

A.3.2 PROOF OF THEOREM 2
Proof. Let x′ ∈ Rn. We will show that with overwhelming probability, ∥x′ − xwat∥ > 1

2εn. Since
the detector outputs true only if ∥x′ − xwat∥ > 1

2εn, this is sufficient to show that any fixed x′ will
not be falsely detected.

Let ∆ = xwat − x, and observe that

∥x′ − xwat∥
2
= ∥x′ − (x+∆)∥2

= ∥∆− (x− x′)∥2

= ∥∆∥2 − 2∆ · (x− x′) + ∥x− x′∥2

Recall that by spherical symmetry of a Gaussian, ∆ · (x − x′) is distributed as ∆1∥x− x′∥ where
∆1 ∼ N (0, ε2). Therefore, with overwhelming probability, 2∆(x−x′) ≤ 2

√
n∥x− x′∥. By Fact 3

and a union bound, with overwhelming probability we also have ∥∆∥2 ≥ 0.9ε2n2. Therefore,

∥x′ − xwat∥
2 ≥ 0.9ε2n2 + ∥x− x′∥(∥x− x′∥ − 2

√
n)

We now consider two cases. If ∥x− x′∥ ≥ 2
√
nn, we have ∥x′ − xwat∥2 ≥ 0.9ε2n2; therefore,

∥x′ − xwat∥ ≥ 1
2εn as desired. If ∥x− x′∥ < 2

√
n, we have ∥x′ − xwat∥2 ≥ 0.9ε2n2 + 4n ≥

0.8ε2n2 for sufficiently large n.

A.3.3 PROOF OF LEMMA 2
Proof. We first show that Assumption 1 implies a useful relationship between (zt − w⃗∗t ) and the
ratio between pit and qit. Observe that

pit =
eℓ

i
t+zt∑

r∈[n] e
ℓir+zr

=
eℓ

i
t+w⃗∗

t +(zt−w⃗∗
t )∑

r∈[n] e
ℓir+zr

≈ eℓ
i
t+w⃗∗

t +(zt−w⃗∗
t )∑

r∈[n] e
ℓir+w⃗∗

r
by Assumption 1

=
eℓ

i
t+w⃗∗

t∑
r∈[n] e

ℓir+w⃗∗
r
· e(zt−w⃗

∗
t )

= qit · e(zt−w⃗
∗
t )

≈ qit + qit · (zt − w⃗∗t ) using the approximation that ex ≈ 1 + x.

Rearranging, we have the useful fact:

(zt − w⃗∗t ) ≈ pit/q
i
t − 1

14



Preprint

where ≈ comes from Assumption 1 and the approximation ex ≈ 1 + x. Thus we have arrived at
Assumption 2.

Armed with Assumptions 1 and 2, and Fact 1, we can prove the lemma. Our goal is to rewrite
the inner product in terms of the expectation of (w⃗wat − w⃗∗)r for tokens r appearing in the given
text. Let T denote the set of tokens over which pi has non-negligible probability. Let pi|T denote the
vector of probabilities when pi|T is restricted only to tokens in T . Let (z−w⃗∗)|T and (w⃗wat−w⃗∗)|T
denote the vectors (z − w⃗∗) and (w⃗wat − w⃗∗) restricted to the tokens in T .

We first manipulate the expression for the dot product and apply Assumption 2:

(z − w⃗∗)|T · (w⃗wat − w⃗∗)|T =
∑
t∈T

(zt − w⃗∗t ) · (w⃗wat − w⃗∗)t

≤
∑
t∈T

(pit/q
i
t − 1) · (w⃗wat − w⃗∗)t + η|(w⃗wat − w⃗∗)t| by Assumption 2

= E
r∼pi|T

[
1

qir
(w⃗wat − w⃗∗)r

]
−
∑
t∈T

(w⃗wat − w⃗∗)t + η|(w⃗wat − w⃗∗)t|

(2)

We now analyze the expression Er∼pi|T

[
1
qir
(w⃗wat − w⃗∗)r

]
to remove the term 1

qir
, which cannot be

determined from just the text. The high entropy and quality of pi lets us do exactly this, invoking
Fact 1.

Recall that Fact 1 says that for the set of tokens T over which pi has non-negligible probability, for
all t ∈ pi we have 1/qit ≤ c1|T |/c2. Let pi|T denote pi restricted to tokens in T and renormalized.
Applying this inequality, we have

E
r∼pi|T

[
1

qir
(w⃗wat − w⃗∗)r

]
=
∑
t∈T

pit(1/q
i
t)(w⃗wat − w⃗∗)t

≤ c1|T |
c2

∑
t∈T

pit(w⃗wat − w⃗∗)t by Fact 1

=
c1|T |
c2

E
r∼pi|T

[(w⃗wat − w⃗∗)r]

Therefore,

E
r∼pi|T

[(w⃗wat − w⃗∗)r] ≥
c2

c1|T |
E

r∼pi|T

[
1

qir
(w⃗wat − w⃗∗)r

]
≥ c2

c1|T |

(
(z − w⃗∗)|T · (w⃗wat − w⃗∗)|T +

∑
t∈T

(w⃗wat − w⃗∗)t

)
− η|(w⃗wat − w⃗∗)t| by Equation (2)

which is a lower bound on this expected value with no dependence on q, as desired.

Finally, we analyze the expected value over the choice of w⃗∗. Recall that from the adversary’s
perspective, w⃗∗ ∼ N (0, ε2I). This immediately lets us simplify the term with η, using the mean of
the folded normal distribution:

E⃗
w∗

[η|(w⃗wat − w⃗∗)t|] = ηε
√
2/π. (3)

As in the proof of Theorem 1, let v = w⃗∗−w⃗wat and let u = z−w⃗wat. Then (z−w⃗∗)·(w⃗wat−w⃗∗) =
(u− v) · v, where v ∼ N (0, ε2I).

15



Preprint

E⃗
w∗

E
r∼pi|T

[(w⃗wat − w⃗∗)r] ≥ E⃗
w∗

[
c2

c1|T |

(
(z − w⃗∗)|T · (w⃗wat − w⃗∗)|T +

∑
t∈T

(w⃗wat − w⃗∗)t − η|(w⃗wat − w⃗∗)t|

)]

= E⃗
w∗

[
c2

c1|T |

(
(z − w⃗∗)|T · (w⃗wat − w⃗∗)|T +

∑
t∈T

(w⃗wat − w⃗∗)t

)]
−

c2ηε
√

2/π

c1
by Equation (3)

= E⃗
w∗

[
c2

c1|T |
(z − w⃗∗)|T · (w⃗wat − w⃗∗)|T

]
−

c2ηε
√
2/π

c1
since each (w⃗wat − w⃗∗)t has mean 0

= E
v∼N (0,ε2I)

[
c2

c1|T |
(u− v)|T · v|T

]
−

c2ηε
√

2/π

c1

= E
v∼N (0,ε2I)

[
c2

c1|T |

(
∥v|T ∥22 − u|T · v|T

)]
−

c2ηε
√
2/π

c1

= E
v∼N (0,ε2I)

[
c2

c1|T |
∥v|T ∥22

]
−

c2ηε
√
2/π

c1
since each ui · vi has mean 0

= c2ε
2/c1 −

c2ηε
√
2/π

c1
Therefore,

E
w⃗∗

r∼pi|T

[w⃗wat − w⃗∗] ≥ c2ε
2/c1 −

c2ηε
√

2/π

c1
.

A.3.4 PROOF OF THEOREM 3
.

Proof. First, observe that Exi∼pi

w⃗∗
[(w⃗wat − w⃗∗)xi ] ≥ 0 for any distribution pi produced by the

adversary. We’ve shown in Lemma 2 that for pi’s that are c1-high entropy and c2-high quality,
Exi∼pi|T

w⃗∗
[(w⃗wat − w⃗∗)xi ] ≥ c2ε

2/c1, where T is the set of tokens with non-negligible probability.

Formally, we model the tokens appearing in the text as sampled from pi|T rather than pi, since the
negligible probability tokens will never be sampled.

By assumption, at least a δ fraction of pi’s have are c1-high entropy and c2-high quality. Therefore,
letting U denote the set of indices of distinct tokens in the response,

1

|U |
E

[∑
i∈U

(w⃗wat − w⃗∗)xi

]
≥ δc2ε

2

c1
− α = 2ε2τtext

By Assumption 3, the (w⃗wat − w⃗∗)xi ’s are independent. Furthermore, all (w⃗wat − w⃗∗)xi ’s lie in
[−λ1/4, λ1/4] with overwhelming probability by a standard Gaussian tail bound (Fact 4):

Pr
[
∃xj ∈ U s.t. ∆(xj) /∈ [−λ1/4, λ1/4]

]
≤ 2|U | exp

(
−λ1/2

2ε2

)
.

Let X =
∑

i∈U (w⃗wat − w⃗∗)xi
. By a Hoeffding bound (Fact 5),

Pr [|X − E[X]| ≥ E[X]/2] ≤ 2 exp

(
−2(E[X]/2)2

|U |(2λ1/4)2

)
≤ 2 exp

(
−2|U |2ε4τ2text

4|U |
√
λ

)
since E[X] ≥ |U |2ε2τtext

= 2 exp

(
−|U |ε4τ2text

2
√
λ

)
which is negligible in λ, as |U | ≥ λ.

16



Preprint

Figure 5: Quality scores of watermarked texts generated with various parameters of epsilon. The
quality score of unwatermarked text was 59.933.

A.3.5 PROOF OF THEOREM 4
Proof. Consider a given text, and recall that our detector only looks at distinct tokens. Therefore,
let x1, . . . , xk denote the given text with the duplicate tokens removed. Observe that for each i ≥ λ,
the count during that iteration is

∑i
j=1 ∆(xj), where each ∆(xj) is an i.i.d. Gaussian N (0, ε2).

Therefore, their sum is also a Gaussian random variable X ∼ N (0, iε).

By Fact 4,

Pr
[
X ≥ iτtextε

2
]
≤ 2 exp

(
−2i2τ2textε

4

2iε2

)
= 2 exp

(
−iτ2textε

2
)

Since i ≥ λ, this is negligible in λ. By a union bound, the probability that this sum exceeds the
threshold for any i is at most k times this expression, which is also negligible.

A.4 ADDITIONAL EXPERIMENT SPECIFICATIONS AND FIGURES

Experiment specifications. We generate responses of up to 300 tokens using random sampling,
at a temperature of 0.9. We set the maximum ngram repeat length to 5, to avoid overly repeti-
tive responses. We generate three types of responses: story responses, essay responses, and code
responses. We use the following prompts:

Story prompt: “Here is one of my favorite stories: It was a ”
Essay prompt: “Here is one of my favorite essays: It is often thought that ”
Code prompt: “Here is a python script for your desired functionality: import ”

To evaluate quality, we ask Mistral-7B-Instruct to evaluate each response based on its type, and to
provide a score out of 100.

Additional figures. We include an additional figure showing the quality of watermarked texts. We
also include larger versions of Figure 3.

17



Preprint

Figure 6: Detection rates for adversarially perturbed watermarked OPT-6.7B models, to simulate a
removal attack. The x-axis plots the epsilon parameter used in the watermark. The three curves show
detection rates for models obtained by adding additional Gaussian noise to the biases, of standard
deviaton 1, 2, and 5 times epsilon. (Figure 3a)

18



Preprint

Figure 7: Quality scores of watermarked texts generated with various parameters of epsilon, under
the same perturbation attacks. The quality score of unwatermarked text was 59.933. (Figure 3b)

19


