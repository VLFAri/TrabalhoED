
Faster Algorithms for User-Level Private Stochastic Convex
Optimization

Andrew Lowy† Daogao Liu‡ Hilal Asi§

October 25, 2024

Abstract

We study private stochastic convex optimization (SCO) under user-level differential privacy
(DP) constraints. In this setting, there are n users (e.g., cell phones), each possessing m data
items (e.g., text messages), and we need to protect the privacy of each user’s entire collection
of data items. Existing algorithms for user-level DP SCO are impractical in many large-scale
machine learning scenarios because: (i) they make restrictive assumptions on the smoothness
parameter of the loss function and require the number of users to grow polynomially with the
dimension of the parameter space; or (ii) they are prohibitively slow, requiring at least (mn)3/2

gradient computations for smooth losses and (mn)3 computations for non-smooth losses. To
address these limitations, we provide novel user-level DP algorithms with state-of-the-art excess
risk and runtime guarantees, without stringent assumptions. First, we develop a linear-time
algorithm with state-of-the-art excess risk (for a non-trivial linear-time algorithm) under a mild
smoothness assumption. Our second algorithm applies to arbitrary smooth losses and achieves
optimal excess risk in ≈ (mn)9/8 gradient computations. Third, for non-smooth loss functions,
we obtain optimal excess risk in n11/8m5/4 gradient computations. Moreover, our algorithms do
not require the number of users to grow polynomially with the dimension.

1 Introduction
The increasing ubiquity of machine learning (ML) systems in industry and society has sparked serious
concerns about the privacy of the personal data used to train these systems. Much work has shown
that ML models may violate individuals’ privacy by leaking their sensitive training data [SSSS17,
LLL+24a, LLL+24b]. For instance, large language models (LLMs) are vulnerable to black-box attacks
that extract individual training examples [CTW+21]. Differential privacy (DP) [DMNS06] prevents
ML models from leaking their training data.

The classical definition of differential privacy—item-level differential privacy [DMNS06]—is ill-
suited for many modern applications. Item-level DP ensures that the inclusion or exclusion of any
one training example has a negligible impact on the model’s outputs. If each person (a.k.a. user)
contributes only one piece of training data, then item-level DP provides a strong guarantee that each
user’s data cannot be leaked. However, in many modern ML applications, such as training LLMs on
users’ data in federated learning, each user contributes a large number of training examples [XZ24].
In such scenarios, the privacy protection that item-level DP provides for each user is insufficiently
weak.

User-level differential privacy is a stronger privacy notion that addresses the above shortcoming
of item-level DP. Informally, user-level DP ensures that the inclusion or exclusion of any one user’s
entire training data (m samples) has a negligible impact on the model’s outputs. Thus, user-level

∗Authors are listed in reverse alphabetical order.
†University of Wisconsin-Madison. alowy@wisc.edu
‡University of Washington. liudaogao@gmail.com
§Apple Machine Learning Research. hilal.asi94@gmail.com

1


DP provides a strong guarantee that no user’s data can be leaked, even when users contribute many
training examples.

A fundamental problem in (private) machine learning is stochastic convex optimization (SCO):
given a data set D = (Z1, . . . , Zn) from n i.i.d. users, each possessing m i.i.d. samples from an
unknown distribution Zi ∼ Pm our goal is to approximately minimize the expected population loss

F (x) := Ez∼P [f(x, z)].

Here, f : X × Z → R is a loss function (e.g., cross-entropy loss), X ⊂ Rd is the parameter domain,
and Z is the data universe. We require that the output of the optimization algorithm A : Zmn → X
satisfies user-level DP (Definition 1.3). We measure the accuracy of A by its excess (population) risk

EF (A(D))− F ∗ := EA,D∼PnmF (A(D))−min
x∈X

F (x).

Given the practical importance of user-level DP SCO, it is unsurprising that many prior works
have studied this problem. The work of [LSA+21] initiated this line of work, and provided an excess
risk lower bound of Ω(1/

√
nm +

√
d/(εn

√
m)), where ε is the privacy parameter. However, their

upper bound was suboptimal and required strong assumptions. The work of [BS23] gave an algorithm
that achieves optimal risk for β-smooth losses with β < (n/

√
md ∧ n3/2/(d

√
m)), provided that

n ≥
√
d/ε and m ≤ max(

√
d, nε2/

√
d). These assumptions are restrictive in large-scale applications

with a large number of examples per user m or when the number of model parameters d is large.
For example, in deep learning, we often have d≫ n and an enormous smoothness parameter β ≫ 1.
Moreover, their algorithm requires mn3/2 gradient evaluations, making it slow when the number of
users n is large.1 The work of [GKK+23] gave another user-level DP algorithm that only requires
n ≥ log(d)/ε, but unfortunately their algorithm does not run in polynomial-time.

To address the deficiencies of previous works on user-level DP SCO, the recent work [AL24]
provided an algorithm that achieves optimal excess risk in polynomial-time, while also only requiring
n ≥ log(md)/ε users. Moreover, their algorithm also works for non-smooth losses. The drawback
of [AL24] is that it is even slower than the algorithm of [BS23]: for β-smooth losses, their algorithm
requires β · (nm)3/2 gradient evaluations; for non-smooth losses, their algorithm requires (nm)3

evaluations.

Evidently, the runtime requirements and parameter restrictions of existing algorithms for user-level
DP SCO are prohibitive in many important ML applications. Thus, an important question is:

Question 1. Can we develop faster user-level DP
algorithms that achieve optimal excess risk without
restrictive assumptions?

Contribution 1. We give a positive answer to Question 1, providing a novel algorithm that achieves
optimal excess risk using max{β1/4(nm)9/8, β1/2n1/4m5/4} gradient computations for β-smooth loss
functions, with any β <∞ (Theorem 3.2). For non-smooth loss functions, our algorithm achieves
optimal excess risk using n11/8m5/4 gradient evaluations for non-smooth loss functions (Theorem 4.1).
Our runtime bounds dominate those of all prior works in every applicable parameter regime, by
polynomial factors in n,m, and d. Moreover, our results only require n1−o(1) ≥ log(d)/ε users. See
Table 1 for a comparison of our results vs. prior works. For example, for non-smooth loss functions,
our optimal algorithm is faster than the previous state-of-the-art [AL24] by a multiplicative factor
of n13/8m7/4. For smooth loss functions, our optimal algorithm is faster than [AL24] by a factor of
(nm)3/8β3/4 (in the typical parameter regime when n7 ≥ m).

1In the introduction, whenever ε does not appear, we are assuming ε = 1 to ease readability. For runtime bounds,
we also assume n = d to further simplify.

2



Loss Function Reference Gradient complexity Assumptions

  -Smooth 

[BS23]  &

[AL24] None

Our Algorithm 3 None

Non-Smooth
[AL24]         None

Our Algorithm 3 
(smoothed)

None

β β ⋅ (mn)3/2
mn3/2

β1/4 ⋅ (mn)9/8 + β1/2n1/4m5/4

(mn)3

n11/8m5/4

β ≤ n /m m ≤ d ≤ n

Figure 1: Optimal algorithms for user-level DP SCO. We omit logarithms, fix L = R = 1 = ε and n = d.

Linear-Time Algorithms The “holy grail” of DP SCO is a linear-time algorithm with optimal
excess risk, which is unimprovable both in terms of runtime and accuracy. In the item-level DP
setting, such algorithms are known to exist for smooth loss functions [FKT20, ZTOH22]. [AL24]
posed an interesting open question: is there a user-level DP algorithm that achieves optimal excess
risk in linear time for smooth functions? For our second contribution, we make progress towards
answering this question.

Existing techniques for user-level DP SCO are not well-suited for linear-time algorithms. Indeed,
the only prior non-trivial linear-time algorithm is the user-level LDP algorithm of [BS23, Algorithm
5].2 Their algorithm can achieve excess risk ≈ 1/

√
nmε+

√
d/(
√
nmε). Unfortunately, however, their

algorithm requires a very stringent assumption on the smoothness parameter β <
√
n3/(md3), which

is unlikely to hold for large-scale ML problems. Further, the result of [BS23] requires the number of
users queried in each round to grow polynomially with the dimension d, and it assumes m < d < n.
These assumptions severely limit the applicability of [BS23, Algorithm 5] in practical ML scenarios.
This leads us to:

Question 2. Can we develop a linear-time user-
level DP algorithm with state-of-the-art excess risk,
without restrictive assumptions?

Contribution 2. We answer Question 2 affirmatively in Theorem 2.1: under a very mild requirement
on the smoothness parameter β <

√
nmd, our novel linear-time algorithm achieves excess risk of

≈ 1/
√
nmε+

√
d/(
√
nmε). Moreover, our algorithm does not require the number of users to grow

polynomially in the dimension d, and our result holds for any values of m, d, and n. Thus, our
algorithm has excess risk matching that of [BS23], but is much more widely applicable.

1.1 Techniques
We develop novel techniques and algorithms to achieve new state-of-the-art results in user-level DP
SCO. Before discussing our techniques, let us review the key ideas from prior works that we build on.

The goal of prior works [BS23, AL24] was to develop user-level analogs of DP-SGD [BFTT19],
which is optimal in the item-level setting. To do so, they observed that each user i’s gradient
1
m

∑m
j=1∇f(x, Zi,j) lies in a ball of radius ≈ 1/

√
m around the population gradient ∇F (x) with

high probability, if the data is i.i.d (Zi ∼ Pm). Consequently, if the data is i.i.d., then replacing one
user Zi ∈ D by another user Z ′

i ∈ D′ will not change the empirical gradient ∇FD(x) by too much:
∥∇FD(x) − ∇FD′(x)∥ ≲ 1/(n

√
m) with high probability. Thus, one would hope for a method to

privatize ∇FD(x) by adding noise that scales with 1/(n
√
m)—rather than 1/n—which would allow

for optimal excess risk. [AL24] devised such a method, which was inspired by FriendlyCore [TCK+22].

2It is trivial to achieve excess risk ≈ 1/
√
nm+

√
d/(εn) with (ε, δ)-user-level, e.g. by applying group privacy to an

optimal item-level DP algorithm such as [FKT20]. The error due to privacy in this bound does not decrease with m.

3



Their method privately detects and removes “outlier” user gradients, and then adds noise to the
average of the “inlier” user gradients. This outlier-removal procedure ensures privacy with noise
scaling with 1/(n

√
m), provided n ≳ 1/ε. Moreover, when the data is i.i.d., no outliers will be

removed with high probability, leading to a nearly unbiased estimator of the empirical gradient.

Our algorithms apply variations of the outlier-removal idea of [AL24] in novel ways.

Our linear-time Algorithm 1 takes a different approach to outlier removal, compared to prior
works. Instead of removing outlier gradients, we aim to detect and remove outlier SGD iterates.3
The high-level idea of our algorithm is to partition the n users into C ≈ 1/ε groups, with each group
containing ≈ nε users. For each group of users, we run T ≈ mnε steps of online SGD using the
samples in this group and obtain the average iterate of each group: {x̃j}Cj=1. We then privately
identify and remove the outlier iterates from {x̃j}Cj=1. In order to successfully do so, we need to argue
that if we run online SGD independently on user Z and user Z ′ to obtain x̃ and x̃′ respectively, then
∥x̃− x̃′∥ ≲ η

√
T with high probability, where η is the SGD step size. We prove such a stability bound

in Lemma 2.3, which we hope will be of independent interest. By repeating the above process log(n)
times and using iterative localization [FKT20], we obtain our state-of-the-art linear-time result.

Our second algorithm, Algorithm 3, builds on [AL24] in a different way. In Algorithm 3, we
apply an outlier-removal procedure to users’ gradients. However, unlike [AL24], we draw random
minibatches of users in each iteration and apply outlier-removal to these minibatches. To make this
procedure private while also achieving optimal excess risk, we combine AboveThreshold [DR14] with
privacy amplification by subsampling [BBG18]. We then develop an accelerated [GL12] user-level
DP algorithm that solves a carefully chosen sequence of regularized ERM problems, and applies
localization in the spirit of [KLL21, AFKT21]. An obstacle that arises when we try to extend the
ERM-based localization framework to the user-level DP setting is getting a tight bound on the
variance of our minibatch stochastic gradient estimator that scales with 1/m. We overcome this
obstacle in Lemma 3.5, by appealing to the stability of user-level DP [BS23]. To handle non-smooth
loss functions, we apply randomized smoothing to our accelerated algorithm.

1.2 Preliminaries
We consider loss functions f : X × Z → R, where X is a convex parameter domain and Z is a data
universe. Let P be an unknown data distribution and F (x) := Ez∼P [f(x, z)] be the population loss
function. Denote F ∗ := minx∈X F (x). The SCO problem is minx∈X F (x). Let ∥ · ∥ denote the ℓ2
norm. ΠX (u) := argminx∈X ∥u− x∥2 denotes projection onto X .

Assumptions and Notation. Function g : X → R is L-Lipschitz if |g(x)− g(x′)| ≤ L∥x− x′∥2
for all x, x′ ∈ X . Function g : X → R is β-smooth if g is differentiable and has β-Lipschitz gradient:
∥∇g(x)−∇g(x′)∥2 ≤ β∥x− x′∥2. Function g : X → R is µ-strongly convex if g(αx+ (1− α)x′) ≤
αg(x) + (1 − α)g(x′) − α(1−α)µ

2 ∥x − x′∥2 for all α ∈ [0, 1] and all x, x′ ∈ X . If µ = 0, we say g is
convex.

Assumption 1.1. 1. The convex set X is compact with ∥x− x′∥ ≤ R for all x, x′ ∈ X .

2. The loss function f(·, z) is L-Lipschitz and convex for all z ∈ Z.

In all of the paper except for Section 4, we will also assume:

Assumption 1.2. The loss function f(·, z) is β-smooth for all z ∈ Z.

Denote a ∧ b := min(a, b). For functions f and g of input parameters θ, we write f ≲ g if there is
an absolute constant C > 0 such that f(θ) ≤ Cg(θ) for all permissible values of θ. We use Õ to hide
logarithmic factors. Write a ≤ poly(b) if there exists some large J > 1 for which a ≤ bJ .

3The reason that this innovation is necessary is discussed in the last paragraph of Section 2.

4



Differential Privacy.

Definition 1.3 (User-Level Differential Privacy). Let ε ≥ 0, δ ∈ [0, 1). Randomized algorithm
A : Znm → X is (ε, δ)-user-level differentially private (DP) if for any two datasets D = (Z1, . . . , Zn)
and D′ = (Z ′

1, . . . , Z
′
n) that differ in one user’s data (say Zi ̸= Z ′

i but Zj = Z ′
j for j ̸= i), we have

P(A(D) ∈ S) ≤ eεP(A(D′) ∈ S) + δ,

for all measurable subsets S ⊂ X .

Definition 1.3 prevents any adversary from learning much more about an individual’s data set
than if that data had not been used for training. Appendix A contains the necessary background on
DP.

1.3 Roadmap
We begin with our state-of-the-art linear-time algorithm in Section 2. In Section 3, we present our
error-optimal algorithm with state-of-the-art runtime for smooth loss functions. Section 4 extends
our fast optimal algorithm to non-smooth loss functions. We conclude in Section 5 with a discussion
and guidance on future research directions stemming from our work.

2 A state-of-the-art linear-time algorithm for user-level DP
SCO

In this section, we develop a new algorithm (Algorithm 1) for user-level DP SCO that runs in
linear time and has state-of-the-art excess risk, without requiring any impractical assumptions. The
algorithm can be seen as a user-level DP variation of the localized phased SGD of [FKT20]: we
execute a sequence of SGD trajectories with geometrically decaying step sizes, shrinking both the
expected distance to the population minimizer and the privacy noise over a logarithmic number of
phases.

In each phase i, we first re-set algorithmic parameters and draw a disjoint set of ni users Di ⊂ D
(lines 4-5). We further partition Di into C disjoint subsets {Di,j}Cj=1. For each j ∈ [C], we pool
together all of the nim samples in Di,j and run one-pass online SGD on Di,j with initial point xi−1

given to us from the previous phase. Next, in lines 10-20, we privately detect and remove “outliers”
from {x̃i,j}Cj=1. That is, our goal is to privately select a subset Si ⊂ {x̃i,j}Cj=1, such that for any two
points x̃i,j , x̃i,j′ ∈ Si, ∥x̃i,j − x̃i,j′∥ ≤ τi = Õ(ηiL

√
Ti). This will enable us to add noise scaling with

τi in line 22, rather than with the much larger worst-case sensitivity (that scales linearly with Ti). In
order to privately select such a subset Si, we first compute (and privatize) the concentration score
for {x̃i,j}Cj=1 in line 10. A small concentration score indicates that outlier removal is doomed to fail
and we must halt the algorithm to avoid breaching the privacy constraint. A large concentration
score indicates that {x̃i,j}Cj=1 is nearly τi-concentrated and we may proceed with outlier removal in
lines 12-15.

Theorem 2.1 (Privacy and utility of Algorithm 1 - Informal). Let ε ≤ 10, n1−o(1) ≳ log(n/δ)
ε ,

β ≤ (L/R)
√
dmnε, and m ≲ poly(n). Then, Algorithm 1 is (ε, δ)-user-level DP. Further,

EF (xl)− F ∗ ≤ LR · Õ

(
1√
nmε

+

√
d log(1/δ)√
nε
√
m

)
.

The gradient complexity of Algorithm 1 is ≤ nm.

Remark 2.2 (State-of-the-art excess risk in linear time, without the restrictive assumptions). Under
the assumptions that β < (Lε3/R)

√
n3/md3 and m ≤ d/ε2 ≤ n, [BS23] gave a linear-time algorithm

with similar excess risk to Algorithm 1. However, their assumptions are very restrictive in practice:

5



Algorithm 1: User-Level DP Phased SGD with Outlier Iterate Removal and Output
Perturbation
1 Input: Dataset D = (Z1, . . . , Zn), privacy parameters (ε, δ), parameters p, q > 0, stepsize η;
2 Set l = ⌊log2(n)⌋, C := 100 log(20nmeε/δ)/ε;
3 for i = 1, · · · , l do
4 Set ni = (1− (1/2)q)n/2iq, ηi = η/2pi, Ni = ni/C, Ti = Nim, τi = 1000ηiL

√
Ti log(ndm);

5 Draw disjoint users Di of size ni from D;
6 Divide Di into C disjoint subsets {Di,j}Cj=1, each containing |Di,j | = Ni users;
7 for j = 1, · · · , C do
8 x̃i,j ← SGD(Di,j , ηi, Ti, xi−1) = average iterate of Ti steps of one-pass projected SGD

with data Di,j , stepsize ηi, and initial point xi−1 ;
9 end

10 Compute the concentration score for Di:

si(τi) :=
1

C

∑
j,j′∈[C]

1(∥x̃i,j − x̃i,j′∥ ≤ τi)

Let ŝi(τi) = si(τi) + Lap(20/ε);
11 if ŝi(τi) ≥ 4C

5 then
12 Si = ∅ ;
13 for j = 1, · · · , C do
14 Compute the score function of x̃i,j : hi,j =

∑C
j′=1 1(∥x̃i,j − x̃i,j′∥ ≤ 2τi);

15 Add x̃i,j to Si with probability pi,j for pi,j =


0 hi,j < C/2

1 hi,j ≥ 2C/3
hi,j−C/2

C/6 o.w.

16 end
17 end
18 else
19 Halt; Output 0
20 end
21 Let x̃i =

1
|Si|
∑

x̃i,j∈Si
x̃i,j ;

22 xi ← x̃i + ζi, where ζi ∼ N (0, σ2
i Id) with σi =

100τi log
2(n/δ)

εC ;
23 end
24 Output: xl.

6



For example, in the canonical regime n ≈ d, their assumption on β rules out essentially every
(non-linear) loss function. By contrast, our result holds even if the smoothness parameter is huge
(β ≈

√
nmd) and we only require a logarithmic number of users. Thus, our algorithm and result is

applicable to many practical ML problems.

To prove that Algorithm 1 is private, we essentially argue that for any phase i, the ℓ2-sensitivity
of x̃i is upper bounded by Õ(τi/C) with probability at least 1− δ/2. The argument goes roughly
as follows: First, the Laplace noise added to si(τi) ensures that si(τi) is ε/4-user-level DP. Now, it
suffices to assume ŝi(τi) ≥ 4C/5, since otherwise the algorithm halts and outputs 0 independently
of the data. Next, conditional on the high probability event that the Laplace noise is smaller than
Õ(1/ε), we know that ŝi(τi) ≥ 4C/5 =⇒ si(τi) ≥ 2C/3 with high probability by our choice of C. In
this case, an argument along the lines of [AL24, Lemma 3.5] shows that x̃i has sensitivity bounded
by Õ(τi/C) with probability at least 1− δ/2. See Appendix B for the detailed proof.

To prove the excess risk bound in Algorithm 1, the key step is to show that if the data is i.i.d.,
then with high probability, no points are removed from {x̃i,j}Cj=1 during the outlier-removal phase
(i.e. |Si| = C). If |Si| = C holds, then we can use the convergence guarantees of SGD and the
localization arguments in [FKT20] to establish the excess risk guarantee. In order to prove that
|Si| = C with high probability, we need the following novel stability lemma:

Lemma 2.3. Assume f(·, z) is convex, L-Lipschitz, and β-smooth on X with η ≤ 1/β. Let
x̃← SGD(D, η, T, x0) and ỹ ← SGD(D′, η, T, x0) be two independent runs of projected SGD, where
D,D′ ∼ PN are i.i.d. Then, with probability at least 1− ζ, we have

∥x̃− ỹ∥ ≲ ηL
√
T log(dT/ζ).

We prove Lemma 2.3 via induction on t, using non-expansiveness of gradient descent on smooth
losses [HRS16], subgaussian concentration bounds, and a union bound.

Lemma 2.3 implies that if the data is i.i.d., then the following events hold with high probability:
∥x̃i,j− x̃i,j′∥ ≤ τi for all j, j′ ∈ [Ci] and hence si(τi) = C. Further, conditional on si(τi) = C, we know
that ŝi(τi) ≥ 4C/5 with high probability, so that the algorithm does not halt. Also, ∥x̃i,j − x̃i,j′∥ ≤ τi
for all j, j′ implies pi,j = 1 for all j and hence |Si| = C for all j. The detailed excess risk proof is
provided in Appendix B.

Challenges of getting optimal excess risk in linear time: In the item-level DP setting,
there are several (nearly) linear time algorithms that achieve optimal excess risk for smooth DP
SCO under mild smoothness conditions, such as snowball SGD [FKT20], phased SGD [FKT20], and
phased ERM with output perturbation [ZTOH22]. Extending these approaches into optimal nearly
linear-time user-level DP algorithms is challenging. First, the user-level DP implementation of output
perturbation in [GKK+23] is computationally inefficient. Second, snowball SGD relies on privacy
amplification by iteration, which does not extend nicely to the user-level DP case due to instability of
the outlier-detection procedure in [AL24]. Specifically, since amplification by iteration intermediate
only provides DP for the last iterate xT but not the intermediate iterates xt (t < T ), the sensitivity of
the concentration score function is not O(1), which impairs DP outlier-detection. A similar instability
issue arises if one tries to naively extend phased SGD to be user-level DP by applying [AL24] to user
gradients. This issue motivates our Algorithm 1, which extends phased SGD in an alternative way:
by applying outlier-detection/removal to the SGD iterates instead of the gradients, we can control
the sensitivity of the concentration score and thus prove that our algorithm is DP. However, since the
bound in Lemma 2.3 scales polynomially with T (and we believe this dependence on T is necessary),
Algorithm 1 adds excessive noise and has suboptimal excess risk. We believe that obtaining optimal
risk in linear time will require a fundamentally different user-level DP mean estimation procedure
that does not suffer from the instability issue.

7



3 An optimal algorithm with ≈ (mn)9/8 gradient complexity
for smooth losses

In this section, we provide an algorithm that achieves optimal excess risk using ≈ (mn)9/8 stochastic
gradient evaluations. Our Algorithm 3 is inspired by the item-level accelerated phased ERM algorithm
of [KLL21]. It applies iterative localization [FKT20] to the user-level DP accelerated minibatch
SGD Algorithm 2. Algorithm 2 is a user-level DP variation of the accelerated minibatch SGD
of [GL12, GL13].

Algorithm 2: User-Level DP Accelerated Minibatch SGD(F̂i, Ti,Ki, xi−1, τ, ε, δ)

1 Initialize x1
i−1 ← xi−1;

2 for t = 1, · · · , Ti do
3 Draw Ki random users Dt

i = {Zt
i,j}

Ki
j=1 from Di uniformly with replacement;

4 Set noisy threshold ∆̂i :=
4Ki

5 + ξi, where ξi ∼ Lap
(
8
ε

)
;

5 Let qt(Z) := 1
m

∑
z∈Z ∇f(xt

i−1, z) for user Z;
6 Compute the concentration score of Dt

i :

sti(τ) :=
1

Ki

∑
Z,Z′∈Dt

i

1(∥qt(Z)− qt(Z
′)∥ ≤ 2τ)

Let ŝti(τ) = sti(τi) + vti , where vti ∼ Lap
(
16
ε

)
;

7 if ŝti(τ) ≥ ∆̂i then
8 Sti = ∅;
9 for Each User Z ∈ Dt

i do
10 Set ht

i(Z) =
∑

Z′∈Dt
i
1(∥qt(Z)− qt(Z

′)∥ ≤ 2τ);

11 Add Z to Sti with probability pti(Z) :=


0 ht

i(Z) < Ki/2

1 ht
i(Z) ≥ 2Ki/3

ht
i(Z)−Ki/2

Ki/6
o.w.

12 end
13 gti =

1
|St

i |
∑

Z∈St
i
∇F̂ (xt

i−1, Z);

14 ĝti = gti + ζti , where ζti ∼ N (0, σ2
i ) with σi =

1000τ
√
Ti log(nde

ε/δ)
εni

;
15 Do 1 iteration of Accelerated Minibatch SGD (AC-SA) [GL12] on F̂i, using gradient

estimator ĝti + λi(x
t
i−1 − xi−1) to obtain xt+1

i−1.
16 end
17 else
18 Halt; Return 0
19 end
20 end
21 Output xTi

i−1.

Our Algorithm 2 applies a DP outlier-removal procedure to the users’ gradients in each iteration.
We use Above Threshold [DR14] to privatize the concentration scores s

(t)
i and determine whether or

not most of the gradients of users in minibatch Dt
i are 2τ -close to each other. If ŝti ≥ ∆̂i, indicating

that the gradients of users in Dt
i are nearly 2τ -concentrated, then we proceed with outlier removal

in lines 8-12. We then invoke privacy amplification by subsampling [BBG18] and the advanced
composition theorem [KOV15] to privatize the average of the “inlier” gradients with additive Gaussian
noise. By properly choosing algorithmic parameters, we obtain the following results, proved in
Appendix C:

8



Algorithm 3: User-Level DP Accelerated Phased ERM with Outlier Gradient Removal
1 Input: Dataset D = (Z1, . . . , Zn), privacy parameters (ε, δ), parameters p, q, λ > 0;
2 Set l = ⌊log2(n)⌋ and τ = O(L log(ndm)/

√
m), choose any initial point x0 ∈ X ;

3 for i = 1, · · · , l do
4 Set ni = (1− (1/2)q)n/2iq, λi = λ · 2pi, Ti = Θ̃(1 +

√
β/λi),

Ki = 500 log(n2
im

2eε/δ)

(
1
ε + niε√

Ti log(1/δ)

)
;

5 Draw disjoint users Di of size ni from D;
6 Let F̂i(x) :=

1
ni

∑
Zi,j∈Di

F̂ (x, Zi,j) + λi

2 ∥x− xi−1∥2, where F̂ (x, Zi,j) is user Zi,j ’s
empirical loss;

7 xi ← User-Level DP Accelerated Minibatch SGD(F̂i, Ti,Ki, xi−1, τ, ε, δ). ;
8 end
9 Output xl.

Theorem 3.1 (Privacy of Algorithm 3). Let ε ≤ 10, q > 0 such that n1−q > 100 log(20nmdeε/δ)
ε(1−(1/2)q) . Then,

Algorithm 3 is (ε, δ)-DP.

Theorem 3.2 (Utility & runtime of Algorithm 3 - Informal). Let ε ≤ 10 and δ < 1/(mn). Then,
Algorithm 3 yields optimal excess risk:

EF (xl)− F ∗ ≤ LR · Õ

(
1√
mn

+

√
d log(1/δ)

εn
√
m

)
.

The gradient complexity of this algorithm is upper bounded by

mn

(
1 + ε

(
βR

L

)1/4
(
(mn)1/8 ∧

(
ε2n2m

d

)1/8
))

+

√
βR

L

(
n1/4m5/4

ε
+

(
n1/2m5/4

d1/4ε1/2

))
.

If n = d, ε = 1, and βR = L then the gradient complexity bound in Theorem 3.2 simplifies to
(mn)9/8 + n1/4m5/4. Typically, n7 ≥ m, so that the dominant term in this bound is (mn)9/8.

Remark 3.3 (State-of-the-art runtime). The gradient complexity bound in Theorem 3.2 is superior
to the runtime bounds of all existing near-optimal algorithms by polynomial factors in n,m, and
d [BS23, GKK+23, AL24]. Note that while the mn3/2 gradient complexity bound of [BS23] may
appear to be better than β1/4(nm)9/8 in certain parameter regimes (e.g. m > n3 or β ≫ nm), this is
not the case: the result of [BS23] requires m < n and β <

√
n/m.

Remark 3.4 (Mild assumptions). Note that Theorems 3.1 and 3.2 do not require any bound on the
smoothness parameter β, and only require the number of users to grow logarithmically: n1−o(1) ≥
Ω̃(1/ε). Contrast this with the results of previous works (e.g. [BS23]).

A challenge in proving Theorem 3.2 is getting a tight bound on the variance of the the noisy
minibatch stochastic gradients ĝti that are used in Algorithm 2 (lines 12-14). Conditional on Sti = Dt

i ,
it is easy to obtain a variance bound of the form E∥ĝti −∇F̂i(x

t
i)∥2 ≲ dσ2

i +
L2

Ki
, since we are sampling

Ki users uniformly at random. However, this bound is too weak to obtain Theorem 3.2, since it does
not scale with m. To prove Theorem 3.2, we need the following stronger result:

Lemma 3.5 (Variance Bound for Algorithm 2). Let δ ≤ 1/(nm), ε ≲ 1. Denote F̃i(x) :=
1
ni

∑
Zi,j∈Di

F̂ (x, Zi,j). Then, conditional on Sti = Dt
i for all i ∈ [l], t ∈ [Ti], we have

E∥gti −∇F̃i(x
t
i−1)∥2 ≲

L2 log(ndm)

Km

for all i ∈ [l], t ∈ [Ti], where the expectation is over both the random i.i.d. draw of D = (Z1, . . . , Zn) ∼
Pnm and the randomness in Algorithm 3.

9



The difficulty in proving Lemma 3.5 comes from the fact that the iterates xt
i and the data D are

not independent. To overcome this difficulty, we use the stability of user-level DP [BS23] to argue
that for all Z ∈ Di, ∇F̂ (xt

i−1, Z) is ≈ L/
√
m-close to ∇F (xt

i−1) with high probability, since xt
i−1 is

user-level DP. A detailed proof is given in Appendix C.

Remark 3.6 (Strongly convex losses: Optimal excess risk with state-of-the-art runtime). If f(·, z) is
µ-strongly convex, then Algorithm 3 can be combined with the meta-algorithm of [FKT20, Section
5.1] to obtain optimal excess risk

L2

µ
· Õ
(

1

nm
+

d ln(1/δ)

ε2n2m

)
with the same gradient complexity stated in Theorem 3.2. This improves over the previous state-of-
the-art gradient complexity ≈ β(mn)3/2 of [AL24].

4 An optimal algorithm with subquadratic gradient complexity
for non-smooth losses

In this section, we extend our accelerated algorithm from the previous section to non-smooth loss
functions. To accomplish this with minimal computational cost, we apply randomized (convolution)
smoothing [YNS12, DBW12] to approximate non-smooth f by a β-smooth f̃ . We can then apply Al-
gorithm 3 to f̃ . Since convolution smoothing is by now a standard optimization technique, we defer
the details and proof to Appendix D.

Theorem 4.1 (Privacy and utility of smoothed Algorithm 3 for non-smooth loss - informal). Let
ε ≤ 10, δ < 1/(mn), and q > 0 such that n1−q > 100 log(20nmdeε/δ)

ε(1−(1/2)q) . Then, applying Algorithm 3 to
the smooth approximation of f yields optimal excess risk:

EF (xl)− F ∗ ≤ LR · Õ

(
1√
mn

+

√
d log(1/δ)

εn
√
m

)
.

The gradient complexity of this algorithm is upper bounded by

mn
(
1 + n3/8m1/4ε1/4

)
.

Remark 4.2 (State-of-the-art gradient complexity). The only previous polynomial-time algorithm
that can achieve optimal excess risk for non-smooth loss functions is due to [AL24]. The algorithm
of [AL24] required (nm)3 + (mn)2

√
d gradient evaluations. Thus, the gradient complexity of the

smoothed version of Algorithm 3 offers a significant improvement over the previous state-of-the-art.
For example, if ε = 1, then our algorithm is faster than the previous state-of-the-art by a multiplicative
factor of at least n13/8m7/4.

5 Conclusion
In this paper, we developed new user-level DP algorithms with improved runtime and excess risk
guarantees for stochastic convex optimization without the restrictive assumptions made in prior
works. Our accelerated Algorithm 3 achieves optimal excess risk for both smooth and non-smooth
loss functions, with significantly smaller computational cost than the previous state-of-the-art. Our
linear-time Algorithm 1 achieves state-of-the-art excess risk under much milder, more practical
assumptions than existing linear-time approaches.

Our work paves the way for several intriguing future research directions. First, the question of
whether there exists a linear-time algorithm that can attain the user-level DP lower bound for smooth
losses remains open. In light of our improved gradient complexity bound (≈ (nm)9/8), we are now

10



optimistic that the answer to this question is “yes.” We believe that our novel techniques will be key
to the development of an optimal linear-time algorithm. Specifically, utilizing Lemma 2.3 to apply
outlier removal to the iterates instead of the gradients appears to be pivotal. Second, the study of
user-level DP SCO has been largely limited to approximate (ε, δ)-DP. What rates are achievable
under the stronger notion of pure ε-user-level DP? Third, it would be useful to develop fast and
optimal algorithms that are tailored to federated learning environments [MRTZ18, GLZW24], where
only a small number of users may be available to communicate with the server in each iteration. We
hope our work inspires and guides further research in this exciting and practically important area.

Acknowledgements
AL’s research is supported by NSF grant 2023239 and the AFOSR award FA9550-21-1-0084. We
thank the anonymous NeurIPS reviewers for their helpful feedback.

11



References
[AFKT21] Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex

optimization: Optimal rates in ℓ1 geometry. In ICML, 2021.

[AL24] Hilal Asi and Daogao Liu. User-level differentially private stochastic convex optimiza-
tion: Efficient algorithms with optimal rates. In International Conference on Artificial
Intelligence and Statistics, pages 4240–4248. PMLR, 2024.

[ALT24] Hilal Asi, Daogao Liu, and Kevin Tian. Private stochastic convex optimization with
heavy tails: Near-optimality from simple reductions. arXiv preprint arXiv:2406.02789,
2024.

[BBG18] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling:
Tight analyses via couplings and divergences. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information
Processing Systems, volume 31. Curran Associates, Inc., 2018.

[BFTT19] Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Thakurta. Private stochastic
convex optimization with optimal rates. In Advances in Neural Information Processing
Systems, volume 32, 2019.

[BS23] Raef Bassily and Ziteng Sun. User-level private stochastic convex optimization with opti-
mal rates. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Con-
ference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,
pages 1838–1851. PMLR, 23–29 Jul 2023.

[CTW+21] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss,
Katherine Lee, Adam Roberts, Tom B Brown, Dawn Song, Ulfar Erlingsson, et al.
Extracting training data from large language models. In USENIX Security Symposium,
volume 6, pages 2633–2650, 2021.

[DBW12] John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for
stochastic optimization. SIAM Journal on Optimization, 22(2):674–701, 2012.

[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of cryptography conference, pages 265–284.
Springer, 2006.

[DR14] Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy,
volume 9. Now Publishers, Inc., 2014.

[FKT20] Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization:
optimal rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium
on Theory of Computing, pages 439–449, 2020.

[GKK+23] Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Raghu Meka, and
Chiyuan Zhang. On user-level private convex optimization. In International Conference
on Machine Learning, pages 11283–11299. PMLR, 2023.

[GL12] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for
strongly convex stochastic composite optimization i: A generic algorithmic framework.
SIAM Journal on Optimization, 22(4):1469–1492, 2012.

[GL13] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for
strongly convex stochastic composite optimization, ii: Shrinking procedures and optimal
algorithms. SIAM Journal on Optimization, 23(4):2061–2089, 2013.

12



[GLZW24] Changyu Gao, Andrew Lowy, Xingyu Zhou, and Stephen J Wright. Private heterogeneous
federated learning without a trusted server revisited: Error-optimal and communication-
efficient algorithms for convex losses. arXiv preprint arXiv:2407.09690, 2024.

[HRS16] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. In Maria Florina Balcan and Kilian Q. Weinberger, editors,
Proceedings of The 33rd International Conference on Machine Learning, volume 48 of
Proceedings of Machine Learning Research, pages 1225–1234, New York, New York, USA,
20–22 Jun 2016. PMLR.

[JNG+19] Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. A short
note on concentration inequalities for random vectors with subgaussian norm. arXiv
preprint arXiv:1902.03736, 2019.

[KLL21] Janardhan Kulkarni, Yin Tat Lee, and Daogao Liu. Private non-smooth erm and sco in
subquadratic steps. Advances in Neural Information Processing Systems, 34:4053–4064,
2021.

[KOV15] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for
differential privacy, 2015.

[LJCJ17] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum
optimization via scsg methods. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pages 2345–2355, 2017.

[LLL+24a] Zhuohang Li, Andrew Lowy, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Bradley
Malin, and Ye Wang. Analyzing inference privacy risks through gradients in machine
learning. arXiv preprint arXiv:2408.16913, 2024.

[LLL+24b] Andrew Lowy, Zhuohang Li, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, and
Ye Wang. Why does differential privacy with large epsilon defend against practical
membership inference attacks? arXiv preprint arXiv:2402.09540, 2024.

[LR22] Andrew Lowy and Meisam Razaviyayn. Private stochastic optimization with large
worst-case lipschitz parameter, 2022.

[LSA+21] Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri,
and Ananda Theertha Suresh. Learning with user-level privacy. Advances in Neural
Information Processing Systems, 34:12466–12479, 2021.

[LUW24] Andrew Lowy, Jonathan Ullman, and Stephen Wright. How to make the gradients small
privately: Improved rates for differentially private non-convex optimization. In Forty-first
International Conference on Machine Learning, 2024.

[McS09] Frank D McSherry. Privacy integrated queries: an extensible platform for privacy-
preserving data analysis. In Proceedings of the 2009 ACM SIGMOD International
Conference on Management of data, pages 19–30, 2009.

[MRTZ18] Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differ-
entially private recurrent language models. In International Conference on Learning
Representations (ICLR), 2018.

[SSSS17] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership
inference attacks against machine learning models. In 2017 IEEE symposium on security
and privacy (SP), pages 3–18. IEEE, 2017.

[SSSSS09] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic
convex optimization. In COLT, volume 2, page 5, 2009.

13



[TCK+22] Eliad Tsfadia, Edith Cohen, Haim Kaplan, Yishay Mansour, and Uri Stemmer. Friendly-
core: Practical differentially private aggregation. In International Conference on Machine
Learning, pages 21828–21863. PMLR, 2022.

[Ull17] Jonathan Ullman. CS7880: rigorous approaches to data privacy, 2017.

[XZ24] Zheng Xu and Yanxiang Zhang. Advances in private training for pro-
duction on-device language models. https://research.google/blog/
advances-in-private-training-for-production-on-device-language-models/,
2024. Google Research Blog.

[YNS12] Farzad Yousefian, Angelia Nedić, and Uday V Shanbhag. On stochastic gradient and
subgradient methods with adaptive steplength sequences. Automatica, 48(1):56–67, 2012.

[ZTOH22] Liang Zhang, Kiran K Thekumparampil, Sewoong Oh, and Niao He. Bring your own
algorithm for optimal differentially private stochastic minimax optimization. Advances
in Neural Information Processing Systems, 35:35174–35187, 2022.

14

https://research.google/blog/advances-in-private-training-for-production-on-device-language-models/
https://research.google/blog/advances-in-private-training-for-production-on-device-language-models/


Appendix

A More Preliminaries

A.1 Tools from Differential Privacy
Additive Noise Mechanisms Additive noise mechanisms privatize a query by adding noise to its
output, with the scale of the noise calibrated to the sensitivity of the query.

Definition A.1 (Sensitivity). Given a function q : ZN → Rk and a norm ∥ · ∥p on Rk, the
ℓp-sensitivity of q is defined as

sup
D∼D′

∥q(D)− q(D′)∥p,

where the supremum is taken over all pairs of datasets that differ in one user’s data.

Definition A.2 (Laplace Distribution). We say X ∼ Lap(b) if the density of X is f(X = x) =
1
2b exp(−

|x|
b ).

Definition A.3 (Laplace Mechanism). Let ε > 0. Given a function q : ZN → Rk on Rk with
ℓ1-sensitivity ∆, the Laplace Mechanism M is defined by

M(D) := q(D) + (Y1, . . . , Yk),

where {Yi}ki=1 are i.i.d., Yi ∼ Lap
(
∆
ε

)
.

Lemma A.4 (Privacy of Laplace Mechanism [DR14]). The Laplace Mechanism is ε-DP.

Definition A.5 (Gaussian Mechanism). Let ε > 0, δ ∈ (0, 1). Given a function q : ZN → Rk with
ℓ2-sensitivity ∆, the Gaussian Mechanism M is defined by

M(D) := q(D) +G

where G ∼ Nk

(
0, σ2Ik

)
and σ2 = 2∆2 log(2/δ)

ε2 .

Lemma A.6 (Privavcy of Gaussian Mechanism [DR14]). The Laplace Mechanism is (ε, δ)-DP.

Advanced Composition If we adaptively query a data set T times, then the privacy guarantees
of the T -th query is still DP and the privacy parameters degrade gracefully:

Lemma A.7 (Advanced Composition Theorem [DR14]). Let ε ≥ 0, δ, δ′ ∈ [0, 1). Assume
A1, · · · ,AT , with At : Zn × X → X , are each (ε, δ)-DP ∀t = 1, · · · , T . Then, the adaptive
composition A(D) := AT (D,AT−1(D,AT−2(X, · · · ))) is (ε′, T δ + δ′)-DP for

ε′ =
√
2T ln(1/δ′)ε+ Tε(eε − 1).

Privacy Amplification by Subsampling

Lemma A.8 ([Ull17]). LetM : ZM → X be (ε, δ)-DP. LetM′ : ZN → X that first selects a random
subsample D′ of size M from the data set D ∈ ZN and then outputsM(D′). Then,M′ is (ε′, δ′)-DP,
where ε′ = (eε−1)M

N and δ′ = δM
N .

AboveThreshold: AboveThreshold algorithm [DR14] which is a key tool in differential privacy to
identify whether there is a query qi : Z → R in a stream of queries q1, . . . , qT that is above a certain
threshold ∆. The AboveThreshold Algorithm 4 has the following guarantees:

Lemma A.9 ([DR14], Theorem 3.24). Let γ > 0 and α = 8 log(2T/γ)
ε , k ∈ [T + 1]. AboveThreshold

is (ε, 0)-DP. Moreover, with probability at least 1− γ, for all t ≤ k, we have:

• if at = ⊤, then qt(D) ≥ ∆− α; and

• if at = ⊥, then qt(D) ≤ ∆+ α.

15



Algorithm 4: AboveThreshold
1 Input: Dataset D = (Z1, . . . , Zn), threshold ∆ ∈ R, privacy parameter ε, sequence of T

queries q1, · · · , qT : Zn → R, each with ℓ1-sensitivity 1;
2 Let ∆̂ := ∆ + Lap( 2ε );
3 for t = 1 to T do
4 Receive a new query qt : Zn → R ;
5 Sample νi ∼ Lap( 4ε );
6 if qt(D) + νi ≥ ∆̂ then
7 Output: at = ⊤;
8 Halt;
9 end

10 else
11 Output: at = ⊥;
12 end
13 end

A.2 SubGaussian and Norm-SubGuassian Random Vectors
Definition A.10. Let ζ > 0. We say a random vector X is SubGaussian (SG(ζ)) with parameter
ζ if E[e⟨v,X−EX⟩] ≤ e∥v∥

2ζ2/2 for any v ∈ Rd. Random vector X ∈ Rd is Norm-SubGaussian with

parameter ζ (nSG(ζ)) if P[∥X − EX∥ ≥ t] ≤ 2e
− t2

2ζ2 for all t > 0.

Theorem A.11 (Hoeffding-type inequality for norm-subGaussian, [JNG+19]). Let X1, · · · , Xk ∈ Rd

be random vectors, and let Fi = σ(x1, ·, xi) for i ∈ [k] be the corresponding filtration. Suppose for
each i ∈ [k], Xi | Fi−1 is zero-mean nSG(ζi). Then, there exists an absolute constant c > 0, for any
γ > 0,

P

∥∥∥∥∥∥
∑
i∈[k]

Xi

∥∥∥∥∥∥ ≥ c

√
log(d/γ)

∑
i∈[k]

ζ2i

 ≤ γ.

Lemma A.12 ([JNG+19]). There exists an absolute constant c, such that if X is nSG(ζ), then for
any fixed unit vector v ∈ Rd, ⟨v,X⟩ is cζ norm-SubGaussian.

B Proof of Theorem 2.1
Theorem B.1 (Formal statement of Theorem 2.1). Suppose n1−q ≥ (100/(1− 1/2q)) log(n/δ)/ε for
some small q > 0, and m ≤ nJ for some large J > 0. Choose p = J +3/2 and η = R/(L

√
dmnε). in

Algorithm 1. Then, Algorithm 1 is (ε, δ)-user-level DP and achieves excess risk

EF (xl)− F ∗ ≤ LR · Õ

(
1√
nmε

+

√
d log(1/δ)√
nε
√
m

)
,

using nm gradient evaluations, provided β ≤ (L/R)
√
dmnε.

The gradient complexity is clear by inspection of the algorithm: The number of stochastic
gradients computed during the algorithm is

l∑
i=1

TiC =

l∑
i=1

NimC =

l∑
i=1

nim ≤ nm.

Next, we will prove the privacy statement in Theorem B.1. The following lemma ensures that if
the Laplace noise added in Algorithm 1 is sufficiently small and outlier detection succeeds, then the
sensitivity of x̃i is Õ(τi/C) with high probability.

16



Lemma B.2. [AL24, Slight modification of Lemma 3.5] Let i ∈ [l] and ζ > 0. Suppose Di and D′
i

differ in the data of one user and we are in phase i of Algorithm 1. Let Ei be the event that the
Laplace noise added to the concentration score si(τi) for Di has absolute value less than 2C/15 and
define E′

i similarly for data D′
i. Denote ai := 1(ŝi(τi) ≥ 4C/5) and a′i := 1(ŝ′i(τi) ≥ 4C/5), where

ŝi(τi) and ŝ′i(τi) are the noisy concentration scores that we get when running phase i of Algorithm 1
on neighboring Di and D′

i, respectively. Then, conditional on ai = a′i and Ei

⋂
E′

i, there is a coupling
Γi over x̃i and x̃′

i such that for (yi, y
′
i) drawn from Γi, we have

∥yi − y′i∥ ≲
τi log(1/ζ)

C

with probability at least 1− ζ.

With Lemma B.2 in hand, we proceed to prove that Algorithm 1 is (ε, δ)-user-level DP:

Proof of Theorem B.1 - Privacy. Privacy: Since the {Di}li=1 are disjoint, parallel composition of
DP [McS09] implies that it suffices to prove that phase i is (ε, δ)-user-level-DP for any fixed i and
fixed xi−1. To that end, let D and D′ be adjacent datasets differing in the data of one user, say
Zi,1 ̸= Z ′

i,1 without loss of generality. We will show that the outputs of phase i when run on D and
D′, xi := xi(D) and x′

i := xi(D′) respectively, are (ε, δ)-indistinguishable.
Let Ei be the event that the Laplace noise added in phase i (for data set D) has absolute value

less than 2C/15 and define E′
i analogously for data set D′. Note that Ei and E′

i are independent and
P(Ei, E

′
i) ≥ 1− δ/10eε. Denote ζ := δ/10eε. Let ai := 1(ŝi(τi) ≥ 4C/5) and a′i := 1(ŝ′i(τi) ≥ 4C/5),

where ŝi(τi) and ŝ′i(τi) are the noisy concentration scores that we get when running phase i of
Algorithm 1 on neighboring Di and D′

i, respectively. By Lemma B.2 and our choice of C, we know
that, conditional on Ei

⋂
E′

i and on ai = a′i, there exists a coupling Γ over (x̃i, x̃
′
i) such that for

(yi, y
′
i) drawn from Γ, we have

∥yi − y′i∥ ≲
τi log(1/ζ)

C
(1)

with probability at least 1− ζ.
Note that the sensitivity of si is less than or equal to 2. Thus, by the privacy guarantees of the

Laplace mechanism (Lemma A.4), we have

P(ai = b) ≤ eε/4P(a′i = b) (2)

for any b ∈ {0, 1}. Further, this implies

P(ai = b, Ei) ≤ eε/4 [P(a′i = b, E′
i) + ζ] . (3)

By the bound (1), the privacy guarantee of the Gaussian mechanism (Lemma A.6), our choice of
σi, and independence of the Laplace and Gaussian noises that we add in Algorithm 1, we have

P(xi ∈ O | Ei, ai = 1) ≤ eε/4P(x′
i ∈ O | E′

i, a
′
i = 1) +

δ

n
+ ζ, (4)

for any event O ⊂ X .
Moreover, since the algorithm halts and returns xi = 0 if ai = 0, we know that

P(xi ∈ O | Ei, ai = 0) = P(x′
i ∈ O | E′

i, a
′
i = 0) (5)

for any event O ⊂ X .
Therefore,

P(xi ∈ O) = P(xi ∈ O | Ei)P(Ei) + P(xi ∈ O | Ec
i )P(Ec

i )

≤ P(xi ∈ O | Ei, ai = 1)P(Ei, ai = 1) + P(xi ∈ O | Ei, ai = 0)P(Ei, ai = 0) + ζ

(i)

≤ eε/4P(x′
i ∈ O | E′

i, a
′
i = 1)eε/4 [P(E′

i, a
′
i = 1) + ζ]

17



+ P(x′
i ∈ O | E′

i, a
′
i = 0)eε/4 [P(E′

i, a
′
i = 0) + ζ] + ζ

≤ eε/2P(x′
i ∈ O, E′

i) + ζ
(
2eε/2 + 1

)
≤ eεP(x′

i ∈ O) + δ,

where (i) follows from inequalities (3), (4), and (5). Thus, xi is (ε, δ)-user-level-DP. This completes
the privacy proof.

Next, we turn to the excess risk proof. The following lemma is immediate from [FKT20, Lemma
4.5]:

Lemma B.3. Let ηi ≤ 1/β. Then, for any y ∈ X and all i, j, we have

E[F (x̃i,j)− F (y)] ≤ E∥y − xi−1∥2

ηiTi
+ ηiL

2.

The next novel lemma is crucial in our analysis:

Lemma B.4 (Re-statement of Lemma 2.3). Assume f(·, z) is convex, L-Lipschitz, and β-smooth on
X with η ≤ 1/β. Let x̃← SGD(D, η, T, x0) and ỹ ← SGD(D′, η, T, x0) be two independent runs of
projected SGD, where D,D′ ∼ PN are i.i.d. Then, with probability at least 1− ζ, we have

∥x̃− ỹ∥ ≲ ηL
√
T log(dT/ζ).

Proof. Let gt := ∇f(xt, zt) for zt drawn uniformly from D without replacement and g′t := ∇f(yt, z′t)
for z′t drawn uniformly from D′ without replacement. Let F (x) := Ez∼P [f(x, z)].

We will prove that ∥xt − yt∥ ≲ ηL
√

T log(dT/ζ) with probability at least 1− ζ/t for all t ∈ [T ].
Note that this implies the lemma. We proceed by induction. The base case, when t = 0, is trivially
true since x0 = y0. For the inductive hypothesis, suppose there is an absolute constant c > 0 such
that with probability at least 1− tζ/T , we have

∥xi − yi∥ ≤ cηL
√
i · log(dT/ζ) + 2ηL,

∀i ≤ t. Then, for the inductive step, we have by non-expansiveness of projection onto convex sets,
that

∥xt+1 − yt+1∥2 ≤ ∥xt − ηgt − (yt − ηg′t)∥2

= ∥xt − η∇F (xt)− (yt − η∇F (yt))− η(gt −∇F (xt)− g′t +∇F (yt))∥2

= ∥xt − η∇F (xt)− (yt − η∇F (yt))∥2

− 2η⟨xt − η∇F (xt)− (yt − η∇F (yt)), gt −∇F (xt)− g′t +∇F (yt)⟩
+ η2∥gt −∇F (xt)− g′t +∇F (yt)∥2

(i)

≤ ∥xt − yt∥2 − 2η⟨xt − η∇F (xt)− (yt − η∇F (yt)), gt −∇F (xt)− g′t +∇F (yt)⟩
+ 4η2L2, (6)

where (i) follows from the non-expansive property of gradient descent on smooth convex function for
η ≤ 1/β [HRS16].

Define at := −2η⟨xt − η∇F (xt)− (yt − η∇F (yt)), gt −∇F (xt)− g′t +∇F (yt)⟩. By Inequality (6)
and the inductive hypothesis, we obtain

∥xt+1 − yt+1∥2 ≤ 4η2L2t+

t∑
i=1

at.

18



It remains to bound
∑t

i=1 ai. Note that E[ai | a1, · · · , ai−1] = 0, and by Lemma A.12 we know there
is a constant c > 0 such that ai is nSG(cηL∥xi − yi∥) for all i. Hence by Theorem A.11, we know

P

∣∣∣∣∣
t∑

i=1

ai

∣∣∣∣∣ ≥ cηL

√
log(dT/γ)

∑
i≤t

∥xi − yi∥2

 ≤ 1− ζ/T.

Conditional on the event that ∥xi − yi∥ ≤ c
√
log(dT/ζ)ηL

√
i for all i ≤ t (which happens with

probability 1− tζ/T by the inductive hypothesis), we know

P

[∣∣∣∣∣
t∑

i=1

ai

∣∣∣∣∣ ≥ c2(t+ 1)L2η2 log(dT/ζ)

∣∣∣∣∣∥xi − yi∥ ≤ c log(dT/ζ)ηL
√
i,∀i ≤ t

]
≤ 1− ζ/T.

Hence we know

P
[
∥xt+1 − yt+1∥2 ≥ c2 log(dT/ζ)η2L2(t+ 1)

∣∣∣∥xi − yi∥ ≤ c log(dT/ζ)ηL
√
i,∀i ≤ t

]
≤ 1− ζ/T.

Combining the above pieces completes the inductive step, showing that ∥xt+1−yt+1∥ ≤ c
√
(t+ 1) log(dT/ζ)ηL+

2ηL with probability at least 1− (t+ 1)ζ/T . This completes the proof.

By combining Lemmas 2.3 and B.3 with the localization proof technique of [FKT20], we can now
prove the excess risk guarantee of Theorem B.1:

Proof of Theorem 2.1 - Excess risk. Excess Risk: First, we will argue that x̃i =
1
C

∑C
j=1 x̃i,j for

all i with high probability ≥ 1− 3/nm. Lemma 2.3 implies that

∥x̃i,j − x̃i,j′∥ ≤ τi

for all i ∈ [l], j, j′ ∈ [C] with probability at least 1 − 1/nm. Thus, si(τi) = C with probability at
least 1− 1/nm. Now, conditional on si(τi) = C, we have ŝi(τi) ≥ 4C/5 for all i with probability at
least 1 − 1/nm by Laplace concentration and a union bound. Moroever, if ∥x̃i,j − x̃i,j′∥ ≤ τi for
all j, j′, then pi,j = 1 for all j and hence there are no outliers: Si = {x̃i,j}j∈[C]. By a union bound,
we conclude that Si = {x̃i,j}j∈[C] and hence x̃i =

1
Si

∑
Di,j∈Si

x̃i,j for all i with probability at least
≥ 1− 3/nm. By the law of total expectation and Lipschitz continuity, it suffices to condition on this
high probability good event that x̃i =

1
C

∑C
j=1 x̃i,j for all i: the total expected excess risk can only

be larger than the conditional excess risk by an additive factor of at most 3LR/nm.
Now, conditional on x̃i =

1
Si

∑
Di,j∈Si

x̃i,j , Lemma B.3 and Jensen’s inequality implies

E[F (x̃i)− F (x̃i−1)] ≲
E∥x̃i−1 − xi−1∥2

ηiTi
+ ηiL

2 =
dσ2

i−1

ηiTi
+ ηiL

2. (7)

Next, let x∗
0 := x∗ = argminx∈XF (x), and write

E[F (xl)− F ∗] =

l∑
i=1

E[F (x∗
i )− F (x∗

i−1)] + E[F (xl)− F (x∗
l )]

≲
R2

ηT1
+ ηL2 +

l∑
i=2

[
ηi−1L

2d+ ηiL
2
]
+ L2

√
dηl
√

Tl

≲
R2

ηT1
+ dηL2 + L2

√
dηl
√
Tl.

Plugging in the prescribed algorithmic parameters completes the excess risk proof.

19



C Proofs of Results in Section 3
Theorem C.1 (Re-statement of Theorem 3.1). Let ε ≤ 10, q > 0 such that n1−q > 100 log(20nmdeε/δ)

ε(1−(1/2)q) .
Then, Algorithm 3 is (ε, δ)-DP.

We require the following lemma, which is a direct consequence of [AL24, Lemma 3.5]:

Lemma C.2. Consider Algorithm 2. Let D′
i and D′

i be two data sets that differ in the data of one
user. Let Ei = {|vti | ≤ Ki/20 ∀t ∈ [Ti] ∩ |ξi| ≤ Ki/20}. Define E′

i similarly for independent draws of
random Laplace noise: E′

i = {|(vti)′| ≤ Ki/20 ∀t ∈ [Ti] ∩ |ξ′i| ≤ Ki/20}. Let ati = 1(ŝti(Di) ≥ 4Ki/5)
and bti = 1(ŝti(D′

i) ≥ 4Ki/5) denote the concentration scores in iteration t. Then, conditional on
Ei

⋂
E′

i and conditional on ati = bti, there exists a coupling Γt
i over gti(Di) and gti(D′

i) such that for
(h, h′) drawn from Γi, we have

∥h− h′∥ ≲ τ log(1/ζ)

Ki

with probability at least 1− ζ.

Proof of Theorem C.1. Note that our assumption on n1−q being sufficiently large implies that ni ≳
log(nmd/δ)

ε for all i ∈ [l]. By parallel composition [McS09] and post-processing, it suffices to show that
{ĝti}

Ti
t=1 satisfies (ε, δ)-user-level DP for any i ∈ [l]. To that end, fix any i ∈ [l] and let D and D′ be

adjacent datasets that differ in the data of one user such that Di ̸= D′
i. We will show that {ĝti(D)}

Ti
t=1

and {ĝti(D′)}Ti
t=1 are (ε, δ)-indistinguishable, which will imply that Algorithm 3 is (ε, δ)-user-level DP.

Let Ei = {|vti | ≤ Ki/20 ∀t ∈ [Ti] ∩ |ξi| ≤ Ki/20}. Define E′
i similarly for independent draws

of random Laplace noise: E′
i = {|(vti)′| ≤ Ki/20 ∀t ∈ [Ti] ∩ |ξ′i| ≤ Ki/20}. Our choice of Ki ≥

500 log(nmdeε/δ)
ε ensures that

P
(
Ei

⋂
E′

i

)
≥ 1− δ/(10eε),

by Laplace concentration and a union bound. Let ζ := δ/(10Tie
ε).

Let ati = 1(ŝti(D) ≥ 4Ki/5) and bti = 1(ŝti(D′) ≥ 4Ki/5). Note that if ati = bti = 0, then
ĝti(D) = 0 = ĝti(D′).

Conditional on the good event that ati = bti for all t and conditional on Ei

⋂
E′

i, we can bound
the ℓ2-sensitivity of gti with high probability, via Lemma C.2 and a union bound:

∥gti(D)− gti(D′)∥ ≲ τ log(1/ζ)

Ki
≲

τ log(nmeε/δ)

Ki
(8)

for all t ∈ [Ti] with probability at least 1− Tiζ = 1− δ/(10eε).
Note that {ŝti(D)}

Ti
t=1 and {ŝti(D′)} are ε/2-indistinguishable by the DP guarantees of AboveThresh-

old in Lemma A.9, since the sensitivity of sti is upper bounded by 2. Therefore,

P({ati}
Ti
t=1 = v,Ei) ≤ eε/2

[
P({bti}

Ti
t=1 = v,E′

i) + ζ
]

(9)

for any v ∈ {0, 1}Ti .

Now, by the sensitivity bound (8), the privacy guarantee of the Gaussian mechanism (Lemma A.6)
and our choice of σi, the advanced composition theorem (Lemma A.7), and privacy amplification by
subsampling (Lemma A.8), we have

P({ĝti(D)}
Ti
t=1 ∈ O | Ei{ati}

Ti
t=1 = v) ≤ eε/2P({ĝti(D′)}Ti

t=1 ∈ O | E′
i, {bti}

Ti
t=1 = v) + (Ti + 1)ζ, (10)

for any event O ⊂ X . Here we also used the fact that Ki ≥ niε√
Ti

.

For short-hand, write {ati}
Ti
t=1 = 1 if ati = 1 for all t ∈ [Ti] and {ati}

Ti
t=1 = 0 if ati = 0 for some

t ∈ [Ti]; similarly for bti. Then since the algorithm halts and returns {ĝti(D)}
Ti
t=1 = 0 if {ati}

Ti
t=1 = 0,

we know that

P({ĝti(D)}
Ti
t=1 ∈ O | Ei, {ati}

Ti
t=1 = 0) = P({ĝti(D′)}Ti

t=1 ∈ O | E′
i, {bti}

Ti
t=1 = 0), (11)

20



for any event O ⊂ X .

Combining the above pieces, we have

P({ĝti(D)}
Ti
t=1 ∈ O) = P({ĝti(D)}

Ti
t=1 ∈ O | Ei)P(Ei) + P({ĝti(D)}

Ti
t=1 ∈ O | Ec

i )P(Ec
i )

≤ P({ĝti(D)}
Ti
t=1 ∈ O | Ei, {ati}

Ti
t=1 = 1)P(Ei, {ati}

Ti
t=1 = 1)

+ P({ĝti(D)}
Ti
t=1 ∈ O | Ei, {ati}

Ti
t=1 = 0)P(Ei, {ati}

Ti
t=1 = 0) + 2Tζ

(i)

≤ eε/2P({ĝti(D′)}Ti
t=1 ∈ O | E′

i, {bti}
Ti
t=1 = 1)eε/4

[
P(E′

i, {bti}
Ti
t=1 = 1) + Tζ

]
+ P({ĝti(D′)}Ti

t=1 ∈ O | E′
i, {bti}

Ti
t=1 = 0)eε/2

[
P(E′

i, {bti}
Ti
t=1 = 0) + Tζ

]
+ Tζ

≤ eε/2P({ĝti(D′)}Ti
t=1 ∈ O, E′

i) + 4Tζ
(
2eε/2 + 1

)
≤ eεP({ĝti(D′)}Ti

t=1 ∈ O) + δ,

where (i) follows from inequalities (9), (10), and (11). Thus, {ĝti(D)}
Ti
t=1 is (ε, δ)-user-level-DP, which

implies the result.

Theorem C.3 (Formal statement of Theorem 3.2). Let ε ≤ 10 and δ < 1/(mn). Then, choosing
λ = L

R

(
1√
nm

+
√
d

εn
√
m

)
and p ≥ 3q + 2.5 + logn(

√
m) in Algorithm 3 yields optimal excess risk:

EF (xl)− F ∗ ≤ LR · Õ

(
1√
mn

+

√
d log(1/δ)

εn
√
m

)
.

The gradient complexity of this algorithm is upper bounded by

mn

(
1 + ε

(
βR

L

)1/4
(
(mn)1/8 ∧

(
ε2n2m

d

)1/8
))

+

√
βR

L

(
n1/4m5/4

ε
+

(
n1/2m5/4

d1/4ε1/2

))
.

We will need the following bound on the excess empirical risk of accelerated (noisy) SGD for the
proof of Theorem C.3:

Lemma C.4. [GL13, Proposition 7] Let xT be computed by T steps of (multi-stage) Accelerated
Minibatch SGD on λ-strongly convex, β-smooth F̂ with unbiased stochastic gradient estimator gt such
that E∥gt −∇F̂ (xt)∥2 ≤ V 2 for all t ∈ [T ]. Then,

E[F̂ (xT )−min
x∈X

F̂ (x)] ≲ [F̂ (x0)−min
x∈X

F̂ (x)] exp

(
−T

√
λ

β

)
+

V 2

λT
.

Remark C.5. As noted in Lemma C.4, we technically need to call a multi-stage implementation
of Algorithm 2 in line 7 of Algorithm 3 (as in [GL13]) to get the desired excess risk bound for
minimizing the regularized ERM problem in each iteration. For improved readability, we omitted
these details in the main body.

Next, we obtain a bound on the variance of the noisy stochastic minibatch gradient estimator ĝti
in Algorithm 2, which can then be plugged in for V 2 in Lemma C.4.

Lemma C.6 (Re-statement of Lemma 3.5). Let δ ≤ 1/(nm), ε ≲ 1. Denote F̃i(x) :=
1
ni

∑
Zi,j∈Di

F̂ (x, Zi,j).
Then, conditional on Sti = Dt

i for all i ∈ [l], t ∈ [Ti], we have

E∥gti −∇F̃i(x
t
i−1)∥2 ≲

L2 log(ndm)

Km

for all i ∈ [l], t ∈ [Ti], where the expectation is over both the random i.i.d. draw of D = (Z1, . . . , Zn) ∼
Pnm and the randomness in Algorithm 3.

21



Proof. By [LJCJ17, Lemma A.1], we know that, conditional on the draw of the data Di and for fixed
xt
i−1, the variance of the minibatch estimator of the gradient of the empirical loss is

E

[∥∥∥gti −∇F̃i(x
t
i−1)

∥∥∥2 ∣∣∣∣∣Di, x
t
i−1

]
= E{il}K

l=1∼Unif([n])


∥∥∥∥∥∥ 1

Km

K∑
l=1

m∑
j=1

∇f(xt
i−1, z

t
il,j

))−∇F̃i(x
t
i−1)

∥∥∥∥∥∥
2 ∣∣∣∣∣Di, x

t
i−1


≤ 1(K = n)

K

1

n

n∑
i=1

∥∥∥∥∥∥ 1

m

m∑
j=1

[∇f(xt
i−1, z

t
i,j)−∇F̃i(x

t
i−1)]

∥∥∥∥∥∥
2

. (12)

Recall F̃i(x) :=
1

nim

∑
z∈Di

f(x, z) is the empirical loss of Di.
Now, for any fixed x and any Z ∈ Di, Hoeffding’s inequality implies that

∥∇F̂ (x, Z)−∇F (x)∥ ≤ τ = O

(
L
√
log(nd/γ)√

m

)

with probability at least 1− γ, where F̂ (x, Z) := 1
m

∑
z∈Z f(x, z) is user Z’s empirical loss. Thus, by

the stability of user-level DP (see [BS23, Theorem 3.4]), for any i ∈ [l], t ∈ [t], we have that

∥∇F̂ (xt
i−1, Z)−∇F (xt

i−1)∥ ≤ τ (13)

for all Z ∈ D with probability at least 1 − γ′ = n(e2εγ + δ), since xt
i−1 is (ε, δ)-user-level DP. To

make γ′ ≲ 1/m, we choose γ = 1/mn and use the assumptions that ε ≲ 1 and δ ≤ 1/mn. Thus, for
any fixed i, t we have

∥∇F̂ (xt
i−1, Z)−∇F (xt

i−1)∥ ≲
L
√

log(n2md)√
m

for all Z ∈ D with probability at least 1− 1/m, which implies

E∥∇F̂ (xt
i−1, Z)−∇F (xt

i−1)∥2 ≲
L2 log(nmd)

m
.

This also implies

E∥∇F̂D(x
t
i−1)−∇F (xt

i−1)∥2 ≲
L2 log(nmd)

m
,

by Jensen’s inequality, where F̂D(x) is the empirical loss over the entire data set D.
Plugging the above bounds into (12) then yields

E∥gti −∇F̃i(x
t
i−1)∥2 ≲

L2 log(ndm)

Km
= ED∼Pnm,{il}K

l=1∼Unif([n])


∥∥∥∥∥∥ 1

Km

K∑
l=1

m∑
j=1

∇f(xt
i−1, z

t
il,j

)−∇F̂D(x
t
i−1)

∥∥∥∥∥∥
2


≲
1(K = n)

K
· L

2 log2(nmd)

m
,

completing the proof.

We are now ready to prove Theorem C.3:

Proof of Theorem C.3. Excess risk: Note that the assumption in the theorem ensures that Ki ≤ ni

for all i. By similar arguments to those used in [AL24, Proposition 3.7], we will show that with high
probability ≥ 1− 2/(nm), for all i ∈ [l], t ∈ [Ti], Sti = Dt

i and hence gti is an unbiased estimator of
∇F̂Dt

i
(xt

i). To show this, first note that for any γ > 0 and any fixed x,

∥∇F̂ (x, Zj)−∇F (x)∥ ≤ L log(nd/γ)√
m

22



with probability at least 1 − γ/Ki by Hoeffding’s inequality (see [AL24, Lemma 4.3]). Next, we
invoke the stability of differential privacy to show that for all t ∈ [Ti], (qt(Z

t
i,1), . . . , qt(Z

t
i,Ki

))

is τ -concentrated (i.e. there exists q∗ ∈ Rd such that ∥qt(Zt
i,j − q∗∥ ≤ τ) with probability at

least 1 − Ti(e
2εγ + δ) (see [BS23, Theorem 4.3]). By a union bound and the choice of γ =

1/[(nm)5/4 log(ndm)e2ε], we have that (qt(Zt
i,1), . . . , qt(Z

t
i,Ki

)) is τ -concentrated for all i ∈ [l], t ∈ [Ti]

with probability at least 1−1/nm. Now, τ -concentration of (qt(Zt
i,1), . . . , qt(Z

t
i,Ki

)) implies sti(τ) = Ki.
Further, sti(τ) = Ki implies ŝti(τ) ≥ 4Ki/5 with probability at least 1− ζ if Ki ≥ 500 log(nm/ζ), by
Laplace concentration and a union bound. Next, note that τ -concentration of (qt(Zt

i,1), . . . , qt(Z
t
i,Ki

))

implies pti(Z
t
i,j) = 1 for all j ∈ [Ki] and Sti = Dt

i . Thus, Sti = Dt
i for all i, t with probability at least

1− 1/nm. Setting ζ = 1/nm and using a union bound shows that with probability at least 1− 2/nm,
we have Sti = Dt

i and gti = ∇F̂Dt
i
(xt

i) for all i, t.
Now, Lemma C.4 implies that, if the outlier-removal procedure in Algorithm 2 leads to an unbiased

gradient estimator gti (line 12) for all t ∈ [Ti = Θ̃(1 +
√

β/λi)], then

E[F̂i(x
Ti
i )−min

x∈X
F̂i(x)] ≲

V 2
i

λiTi
, (14)

where V 2
i = maxt∈[ti] E∥ĝti −∇F̂i(x

t
i)∥2 ≲ dσ2

i +
log(ndm)L2

Kim
(unconditionally, after taking expectation

over the random draw of D ∼ Pnm, by Lemma 3.5). We have shown that the event GOOD :=

{gti = ∇F̂Dt
i
(xt

i) for all i ∈ [l], t ∈ [Ti]} occurs with probability at least 1− 2/nm. We will condition
on GOOD for the rest of the proof: note that the Lipschitz assumption implies that the total
(unconditional) excess risk will only by larger than the conditional (on GOOD) excess risk by an
additive factor of at most 2LR/

√
nm.

By stability of regularized ERM (see [SSSSS09]), we have

E[F (x∗
i )− F (y)] ≲

L2

λinim
+ λiE[∥xi−1 − y∥2] (15)

for all i, where x∗
i := argminxF̂i(x). By strong convexity and (14), we have

(λi/2)E∥xi − x∗
i ∥2 ≤ EF̂i(xi)− F̂ ∗

i ≲
dσ2

i

λiTi
+

L2 log(ndm)

λiTiKim
. (16)

Thus,

E∥xi − x∗
i ∥2 ≲

dσ2
i

λiTi
+

L2 log(ndm)

λiTiKim
≲

dτ2 log(1/δ)

λ2
i ε

2n2
i

+
L2 log(ndm)

λ2
iTiKim

(17)

Now, letting x∗
0 := x∗ = argminx∈XF (x) and hiding logarithmic factors, we have:

E[F (xl)− F ∗] =

l∑
i=1

E[F (x∗
i )− F (x∗

i−1)] + E[F (xl)− F (x∗
l )]

≲
L2

λ1n1m
+ λ1R

2 +

l∑
i=2

E
[

L2

λinim
+ λi∥xi−1 − x∗

i−1∥2
]
+ LE∥xl − x∗

l ∥

≲
L2

λnm
+ λR2 +

l∑
i=2

[
L2

λinim
+ λi

(
dτ2 log(1/δ)

λ2
i−1ε

2n2
i−1

+
L2

λ2
i−1Ti−1Ki−1m

)]

+ L

√
dτ
√

Tl log(1/δ)

λlεnl
,

where the first inequality used (15) and Lipschitz continuity, the second inequality used (17).
Note that KiTi ≥ ni. Further, our choice of sufficiently large p makes λl large enough that

L
√
dτ
√

Tl log(1/δ)

λlεnl
≤ LR

√
d

εn
√
m

. Therefore, upper bounding the sum by it’s corresponding geometric series
gives us

E[F (xl)− F ∗] ≲
LR
√
d

εn
√
m

+
L2

λ

(
1

nm
+

dτ2 log(1/δ)

ε2n2

)
+ λR2. (18)

23



Plugging in λ completes the excess risk proof.
Gradient Complexity: The gradient complexity is

∑l
i=1 TiKim. Plugging in the prescribed

choices of Ti and Ki completes the proof.

D Details on the non-smooth algorithm and the proof of The-
orem 4.1

For any loss function f(·, z), we define the convolution function fr(·, z) := f(·, z) ∗ nr where nr is the
uniform density in the ℓ2 ball of radius r centered at the origin in Rd. Specifically, nr(y) =

Γ( d
2+1)

π
d
2 rd

for ∥y∥ ≤ r, and nr(y) = 0 otherwise. For simplicity, we omit the dependence on z in the following
Lemma:

Lemma D.1 (Randomized Smoothing, [YNS12, DBW12]). For any r > 0, let Xr := X + {x ∈ Rd :
∥x∥ ≤ r}. If f is convex and L-Lipschitz over Xr, then the convolution function fr has the following
properties:

• fr(x) ≤ f(x) ≤ fr(x) + Lr, for all x ∈ X .

• fr is L-Lipschitz and convex.

• fr is L
√
d

r -smooth.

• For random variables y ∼ nr, we have Ey[∇f(x+ y)] = ∇fr(x).

The following lemma can be easily seen from the proofs of Theorems 3.1 and 3.2:

Lemma D.2 (Privacy and utility of Algorithm 3 for general Ki, Ti). Let ε ≤ 10, q > 0 such that
n1−q > 100 log(20nmdeε/δ)

ε(1−(1/2)q) .

• If Ki ≳
niε√
Ti

+ log(nmdeε/δ)
ε , then Algorithm 3 is (ε, δ)-user-level DP.

• If TiKi ≥ ni and Ti ≳ (1+
√
β/λi) log(ndm) for all i, then Algorithm 3 achieves optimal excess

risk.

Theorem D.3 (Formal statement of Theorem 4.1). Let ε ≤ 10, δ < 1/(mn), and q > 0 such
that n1−q > 100 log(20nmdeε/δ)

ε(1−(1/2)q) . Suppose that for any z, f(, z) is convex and L-Lipschitz over Xr

for Xr := X + {x ∈ Rd : ∥x∥ ≤ r} where r =
√
d

εn
√
m
R. Then, running Algorithm 3 with functions

{fr(x; z)}z∈D yields optimal excess risk:

EF (xl)− F ∗ ≤ LR · Õ

(
1√
mn

+

√
d log(1/δ)

εn
√
m

)
.

The gradient complexity of this algorithm is upper bounded by

mn
(
1 + n3/8m1/4ε1/4

)
.

Proof. By Lemma D.1 and our choice of r, we have |fr(x, z) − f(x, z)| ≤ Lr = O(LR
√
d

εn
√
m
). Set

λ = 1√
mn

. Then we know that

EF (xl)− F ∗ ≤E [Fr(xl)− F ∗
r ] +O(LR

√
d

εn
√
m
).

24



Further, Fr is β-smooth for β ≤ L
Rεn
√
m. Set Ti = (1+

√
β/λi) log(ndm) = 1+n

3/4
i m1/2ε1/2 log(ndm)

and Ki =
niε√
Ti

+ log(nmdeε/δ)
ε . Then Lemma D.2 implies that Algorithm 3 is (ε, δ) user-level DP, and

yields the excess risk bound

EFr(xl)− F ∗
r ≤ LR · Õ

(
1√
mn

+

√
d log(1/δ)

εn
√
m

)
,

as desired. The number of gradient evaluations is

l∑
i=1

TiKim ≲ mn
(
1 + n3/8m1/4ε1/4

)
.

This completes the proof.

E Limitations
Our work weakens the assumptions on the smoothness parameter and the number of users that are
needed for user-level DP SCO. Nevertheless, our results still require certain assumptions that may
not always hold in practice. For example, we assume convexity of the loss function. In deep learning
scenarios, this assumption does not hold and our algorithms should not be used. Thus, user-level
DP non-convex optimization is an important direction for future research [LUW24]. Furthermore,
the assumption that the loss function is convex and uniformly Lipschitz continuous may not hold in
certain applications, motivating the future study of user-level DP stochastic optimization with heavy
tails [LR22, ALT24].

Our algorithms are also faster than the previous state-of-the-art, including a linear-time Algo-
rithm 1 with state-of-the-art excess risk. However, our error-optimal accelerated Algorithm 3 runs in
super-linear time. Thus, in certain applications where a linear-time algorithm is needed due to strict
computational constraints, Algorithm 1 should be used instead.

F Broader Impacts
Our work on differentially private optimization for machine learning advances the field of privacy-
preserving ML by developing techniques that protect the privacy of individuals (users) who contribute
data. The significance of privacy cannot be overstated, as it is a fundamental right enshrined in
various legal systems worldwide. However, the implications of our work extend beyond its intended
benefits, and it is essential to consider both potential positive and negative impacts.

Positive Impacts:

1. Enhanced Privacy Protections: By incorporating differential privacy into machine learning
models, we can provide strong privacy guarantees for individuals, mitigating the risk of personal
data being exposed or misused.

2. Ethical Data Utilization: DP ML enables organizations to leverage data while adhering to
ethical standards and privacy regulations, fostering trust among users and stakeholders.

3. Broad Applications: The techniques we develop can be applied across diverse domains, in-
cluding healthcare, finance, and social sciences, where sensitive data is prevalent. This broad
applicability can drive innovations while maintaining privacy.

4. Educational Advancement: Our research contributes to the growing body of knowledge in
privacy-preserving technologies, serving as a valuable resource for future studies and fostering
an environment of continuous improvement in privacy practices.

25



Potential Negative Impacts:

1. Misuse by Corporations and Governments: There is a risk that our algorithms could be exploited
by entities to justify the unauthorized collection of personal data under the guise of privacy
compliance. Vigilant oversight and clear regulatory frameworks are necessary to prevent such
abuses.

2. Decreased Model Accuracy: While DP ML provides privacy benefits, it can also lead to
reduced model accuracy compared to non-private models. This trade-off might have adverse
consequences, such as less accurate medical diagnoses or flawed economic forecasts. For example,
an overly optimistic prediction of environmental impacts due to lower accuracy could be misused
to weaken environmental protections.

While recognizing the potential for misuse and the trade-offs involved, we firmly believe that
the advancement and dissemination of differentially private machine learning algorithms offer a net
benefit to society. By addressing privacy concerns head-on and advocating for responsible use, we
aim to contribute positively to the field of machine learning and uphold the fundamental right to
privacy. Through ongoing research, collaboration, and education, we strive to enhance both the
capabilities and ethical foundations of machine learning technologies.

26


	Introduction
	Techniques
	Preliminaries
	Roadmap

	A state-of-the-art linear-time algorithm for user-level DP SCO
	An optimal algorithm with (mn)9/8 gradient complexity for smooth losses
	An optimal algorithm with subquadratic gradient complexity for non-smooth losses
	Conclusion
	More Preliminaries
	Tools from Differential Privacy
	SubGaussian and Norm-SubGuassian Random Vectors

	Proof of thm: phased erm without reg
	Proofs of Results in Section 3
	Details on the non-smooth algorithm and the proof of Theorem 4.1
	Limitations
	Broader Impacts

